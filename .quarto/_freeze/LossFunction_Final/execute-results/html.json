{
  "hash": "46b76eed9848f6020304b8a1410620f2",
  "result": {
    "engine": "knitr",
    "markdown": "# (2) Loss functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(ggplot2)\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n:::\n\n\nLoad diabetes dataset (already available by installing package [mlbench](https://mlbench.github.io)). This is a toy dataset that has been extensively used in many [machine learning examples](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"PimaIndiansDiabetes\")\n```\n:::\n\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\n\nLets now select only two of this columns `age` and `glucose` and store it as a new dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nData <- PimaIndiansDiabetes %>%\n        select(age, glucose)\n```\n:::\n\n\nRecall this is the same thing as\n\n\n::: {.cell}\n\n```{.r .cell-code}\nData <- PimaIndiansDiabetes[, c(\"age\", \"glucose\")]\n```\n:::\n\n\nWe have *768* observations/rows, so lets cut it down to just 30, for the sake of easier visualization and take a look\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataSmaller <- Data[1:80,]\n\nhead(DataSmaller)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age glucose\n1  50     148\n2  31      85\n3  32     183\n4  21      89\n5  33     137\n6  30     116\n```\n\n\n:::\n:::\n\n\n\n## Define \"Best Fit\" – Minimizing Error\n\nThe *\"best fit\"* line minimizes the average distance (error) between the predicted values and the actual data points. This error (or residual) for a single data point is calculated as:\n\n$$\nResidual = ActualValue - Predicted Value\n$$\n\n## Example with a Single Data Point\n\nLet's calculate the residual for a single data point in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataSmaller <- Data[1:80,]\n# Calcualte error of line (y = 112 + 0.5x)\nDataSmaller$line1 <- 120 + 0.5 * seq(0, 60, length.out = 80)\nDataSmaller$linePredicted <- 120 + 0.5 *DataSmaller$age\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](LossFunction_Final_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataSmaller$linePredicted <- 120 + 0.5 *DataSmaller$age\n# Get the predicted value for the first data point\npredicted_value <- DataSmaller$linePredicted[63]\n\n# Calculate the residual\nactual_value <- DataSmaller$glucose[63]\nresidual <- actual_value - predicted_value\n\n# Print results\ncat(\"Actual Value:\", actual_value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nActual Value: 44 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Predicted Value:\", predicted_value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted Value: 138 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual (Error):\", residual, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual (Error): -94 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n   geom_point(aes(x = DataSmaller$age[63], y = actual_value), size = 3, color = \"darkred\", alpha = 0.7) +\n   geom_point(aes(x = DataSmaller$age[63], y = predicted_value), size = 3, color = \"orange\", alpha = 0.7) +\n  geom_segment(aes(x = DataSmaller$age[63], y = actual_value, xend = DataSmaller$age[63], yend = predicted_value),\n               color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Use of `DataSmaller$age` is discouraged.\nℹ Use `age` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_point(aes(x = DataSmaller$age[63], y = actual_value), size = 3, : All aesthetics have length 1, but the data has 80 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Use of `DataSmaller$age` is discouraged.\nℹ Use `age` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_point(aes(x = DataSmaller$age[63], y = predicted_value), : All aesthetics have length 1, but the data has 80 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Use of `DataSmaller$age` is discouraged.\nℹ Use `age` instead.\nUse of `DataSmaller$age` is discouraged.\nℹ Use `age` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_segment(aes(x = DataSmaller$age[63], y = actual_value, xend = DataSmaller$age[63], : All aesthetics have length 1, but the data has 80 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](LossFunction_Final_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Sum of Errors – squares or absolute value?\n\nErrors can be both positive and negative, so simply summing them would lead to cancellation, which isn’t meaningful. Two common methods are:\n\n- __1.Sum of Squared Residuals__: This squares each error, avoiding cancellation and penalizing larger errors more.\n\n- __2.Absolute Values of Residuals__: Taking absolute values also prevents cancellation, but doesn’t penalize large errors as heavily as squaring.\n\n## Different Error Metrics\nLet's calculate different error metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate Residuals\nresiduals <- DataSmaller$glucose - DataSmaller$linePredicted\n\n# Define a function to calculate error metrics\ncalculate_errors <- function(residuals) {\n  \n  # Sum of Squared Residuals (SSR)\n  SSR <- sum(residuals^2)\n  \n  # Mean Squared Error (MSE) – equivalent to L2 loss\n  MSE <- (SSR)/dim(DataSmaller)[1]\n  \n  # Root Mean Squared Error (RMSE)\n  RMSE <- sqrt(MSE)\n  \n  # Mean Absolute Error (MAE) – equivalent to L1 loss\n  MAE <- mean(abs(residuals))\n  \n  # Print the results\n  cat(\"Sum of Squared Residuals (SSR):\", SSR, \"\\n\")\n  cat(\"Mean Squared Error (MSE):\", MSE, \"\\n\")\n  cat(\"Mean Absolute Error (MAE):\", MAE, \"\\n\")\n  \n  # Return a list of the error metrics\n  return(list(SSR = SSR, MSE = MSE, RMSE = RMSE, MAE = MAE))\n}\n\nerror_metrics <- calculate_errors(residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of Squared Residuals (SSR): 111807 \nMean Squared Error (MSE): 1397.588 \nMean Absolute Error (MAE): 30.575 \n```\n\n\n:::\n:::\n\n\n## Choosing a loss\n\nDeciding whether to use MAE or MSE can depend on the dataset and the way you want to handle certain predictions. Most feature values in a dataset typically fall within a distinct range.Values outside the typical range and would be considered an __outlier__.\n\nWhen choosing the best loss function, consider how you want the model to treat outliers. For instance, __MSE moves the model more toward the outliers, while MAE doesn't__. L2 loss incurs a much higher penalty for an outlier than L1 loss. \n\n![MSE vs MAE](/Users/bravol/Desktop/ML Class/Practical/LinearReg_Lecture/MSE_MAE.png){fig-align=\"center\"}\n\n\nRegardless, the functions that we will use that implement linear regression algorithms (e.g `lm()`) take into account MSE error, so this will not be part of any decision we have to take. The reason for this is that MSE has benefits that MAE has not in terms of *optimizimg* it! We will learn about this later.  \n\n### Outliers\n\nIn data pre-processing we discussed *outliers*, and here we will try and visually understand their influence when modeling. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear model\nmodel_MSE <- lm(glucose ~ age, data = DataSmaller)\nDataSmaller$predictions_MSE <- predict(model_MSE, DataSmaller)\n\nprint(model_MSE$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)         age \n  73.677460    1.330072 \n```\n\n\n:::\n:::\n\n\nCalculate residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals_MSE <- DataSmaller$glucose - DataSmaller$predictions_MSE\nresiduals_MSE <- model_MSE$residuals # can also extract them directly from the model!\n```\n:::\n\n\nCalculate all losses \n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror_metrics_MSE <- calculate_errors(residuals_MSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of Squared Residuals (SSR): 81632.69 \nMean Squared Error (MSE): 1020.409 \nMean Absolute Error (MAE): 24.23672 \n```\n\n\n:::\n:::\n\n\nPlot linear regression model\n\n::: {.cell}\n\n```{.r .cell-code}\nMSE <- ggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE)[1], slope = coef(model_MSE)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE without Outliers\", x = \"Age\", y = \"Glucose\")\n```\n:::\n\n\nAdd Outliers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Introduce outliers \n\nDataOutliers <- DataSmaller\nDataOutliers$glucose[c(1, 3, 5)] <- DataOutliers$glucose[c(1, 3, 5)] * 3 # Changing 3 readings into 3 times their value! \n\nmodel_MSE_out <- lm(glucose ~ age, data = DataOutliers)\nDataOutliers$predictions_MSE_out <- predict(model_MSE_out, DataOutliers)\n\n#Calculate residuals\n\nresiduals_MSE_out <- DataOutliers$glucose - DataOutliers$predictions_MSE_out\nresiduals_MSE_out <- model_MSE_out$residuals # can also extract them directly from the model!\n\n#Calculate loss \n\nerror_metrics_MSE_out <- calculate_errors(residuals_MSE_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of Squared Residuals (SSR): 430818.6 \nMean Squared Error (MSE): 5385.232 \nMean Absolute Error (MAE): 38.6216 \n```\n\n\n:::\n\n```{.r .cell-code}\nMSE_Out <- ggplot(DataOutliers, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE_out)[1], slope = coef(model_MSE_out)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE Outliers\", x = \"Age\", y = \"Glucose\")\n```\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nMSE + MSE_Out \n```\n\n::: {.cell-output-display}\n![](LossFunction_Final_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n---------\n\n\n\nAs key learning points:  MAE vs MSE?\n\n- *Look at the scales!* - MAE is more interpretable as it gives a more straightforward interpretation of the \"average error,\" as it represents the median prediction error.\n- Robustness to Outliers: MAE is less sensitive to outliers than MSE because it doesn't square the errors.\nBut the functions used to implement linear regression use mostly MSE, because MAE is not differentiable, so it requires specialised optimisation algorithms, which are less computationally efficient than least-squares (MSE) for large datasets as MSE is differentiable and so easier to optimize. (we will learn about what this means later!)\n\n",
    "supporting": [
      "LossFunction_Final_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "fe0d7d2b3e8255872bfcf9dec4fe7eb7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"(1) Supervised Learning Models\"\nformat: html\n---\n\n**Mention that you can just copy the code and insert it as an Rmd!**\n\nSupervised learning models can make predictions after seeing lots of data with the correct answers and then discovering the connections between the elements in the data that produce the correct answers. This is like a student learning new material by studying old exams that contain both questions and answers. Once the student has trained on enough old exams, the student is well prepared to take a new exam. *These ML systems are \"supervised\" in the sense that a human gives the ML system data with the known correct results*\n\nPackages needed for this session:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench) #where PimaIndiansDiabetes database is found\nlibrary(tidyverse) #for minimal data wrangling \nlibrary(ggplot2) #already part of tidyverse - so a bit redundant\nlibrary(tidymodels) #for modeling\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n:::\n\n\nLoad diabetes dataset (already available by installing package [mlbench](https://mlbench.github.io)). As described yesterday, this is a very popular dataset and examples of its use can be found here: [machine learning examples](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"PimaIndiansDiabetes\")\n```\n:::\n\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\n\nLets now select only two of this columns `age` and `glucose` and store it as a new dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nData <- PimaIndiansDiabetes %>%\n        select(age, glucose)\n```\n:::\n\n\nWe have *768* observations/rows, so lets cut it down to just 80, for the sake of easier visualization and take a look\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataSmaller <- Data[1:80,]\n\nhead(DataSmaller)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age glucose\n1  50     148\n2  31      85\n3  32     183\n4  21      89\n5  33     137\n6  30     116\n```\n\n\n:::\n:::\n\n\nNow lets visualize this relationship\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\")\n```\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n## A Supervised Learning Algorithm: Linear Regression\n\nSuppose we want to predict our glucose level based on our age with this dataset. We could fit a learning algorithm to our dataset and create a predictive model. In this case the *learning algorithm* is going to be *linear regression*. ggplot already contains the functionality of fitting and visualizing this linear regression model through `geom_smooth` as seen below (this is the solution from the practical from the first day):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_smooth(method='lm', se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThis amounts to using the `lm` function to fit the data as you have seen in the Statistics Module [statistics recap](https://anasrana.github.io/ems-practicals/linear-regression.html)\n\nTo recap, the linear regression algorithm is as follows:\n\nIn ML, we write the equation as\n\n$$\ny = \\beta_0 + \\beta_1\\, x_1 \n$$\n\nwhere\n\n-   y = outcome label ( column to predict )\n-   \\beta\\_0 = sometimes known as bias, it is the intercept of the line and is a *parameter* of the model\n-   \\beta\\_1 = weight of the feature/column/x 1 - same as slope in equation of a line (if we only had 2 dimensions) and is a *parameter* of the model\n-   x_1 = feature/column 1/ input data\n\nAnd how do we find out the two parameters? They are the result of *fitting* the learning algorithm (linear regression) to the data. Once we have them defined, we can then say we have the predictive algorithm where, if we were to have a new sample ($x_i$) with age information, we could find out the predicted glucose ($y_i'$).\n\nAs you will see later, the way this *fitting* works, is by finding the parameters \\beta\\_0 and \\beta\\_1 which make the best fit line.And this is calculated through *optimization* of the *loss/cost function* through either *ordinary least squares (OLS)* or *gradient descent* (do not worry about this just now!)\n\nMore specifically, when you use the function below `linear_reg()` or any other function that fits a linear model in R or Python, \\beta\\_0 and \\beta\\_1 or \\beta\\_n (depending on the amount of features you are including in your model) are being calculated using *OLS*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- linear_reg() %>% \n            set_engine('lm') %>% # adds lm implementation of linear regression\n            set_mode('regression')\n\n\nls_fit <- lm_model %>% \n          fit(glucose ~ age, DataSmaller)\n```\n:::\n\n\n```{ls_fit[[\"fit\"]][[\"coefficients\"]]}\n```\n\n```{}\n```\n\nIf we include this coefficients and draw the line ourselves, we will obtain the same thing as the `geom_smooth` above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| fig-align: 'center'\n#| fig-width: 6\n#| fig-height: 4\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = ls_fit[[\"fit\"]][[\"coefficients\"]][1], slope = ls_fit[[\"fit\"]][[\"coefficients\"]][2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe predictive model then (with fitted parameters) is:\n\n$$\nglucose = 73.68 + 1.33*age\n$$ If we now have a new sample coming with $age = 45$, what is the predicted glucose? Can you include it in the plot?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_newsample<- 45\npredicted_glucose <-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 133.53\n```\n\n\n:::\n:::\n\n\nThis is the same as doing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(ls_fit, new_data = data.frame(age = 45)) # we had to create a new dataframe with one value of age for this to work, as it normally expects more than 1 smple, and more features!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  134.\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n```\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nQuestion!! If my new sample is $age = 10$, can I still predict glucose, even though none of my data used to create the model had a sample with age = 10?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_newsample<- 10\npredicted_glucose <-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 86.98\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) + \n  xlim(0, 100)\n```\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nYes we can! Another thing is how confident we are about the predicted value, but we will discuss that later on.The model is an infinite line, mapping age to glucose (so we could even map negative values, or very high age values (e.g 1000) and obtain a predicted glucose - but it would make no sense).\n\n### EXERCISE: Understand the role of the dataset in predictive modelling\n\nLets fit the decision tree learning algorithm in different number of training points (you will learn more about how decision trees work next Tuesday, but as you have seen in the previous classes, learning algorithms are already coded for you as functions, so you can apply them already)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndt_model <- decision_tree() %>% \n            set_engine('rpart') %>% # check out what the rpart library/engine is about\n            set_mode('regression')\n\n\ndt_fit <- dt_model %>% \n          fit(glucose ~ age, DataSmaller)\n```\n:::\n\n\nCheck out what the fitted model consist of, and print out the information the model encodes (open ls_fit too and see)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#View(dt_fit)\ndt_fit[[\"fit\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 80 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 80 99307.69 120.5625  \n   2) age< 22.5 10  8122.90  78.9000 *\n   3) age>=22.5 70 71347.49 126.5143  \n     6) age< 38.5 42 34265.90 117.9524  \n      12) age>=26.5 31 22403.87 114.9355 *\n      13) age< 26.5 11 10784.73 126.4545 *\n     7) age>=38.5 28 29384.43 139.3571  \n      14) age< 50.5 16 16919.75 132.6250 *\n      15) age>=50.5 12 10772.67 148.3333 *\n```\n\n\n:::\n:::\n\n\nNow lets plot the model to get a better idea of what it is doing ( this would be the same as priniting out the intercept and coefficients for a linear regression algorithm )\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: rpart\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'rpart'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dials':\n\n    prune\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the decision tree\nrpart.plot(dt_fit[[\"fit\"]], type = 3, fallen.leaves = TRUE, box.palette = \"Blues\", shadow.col = \"gray\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Get the predicted glucose values \nDataSmaller$predicted_glucose <- predict(dt_fit, new_data = DataSmaller)$.pred\n```\n:::\n\n\nThis we are now introducing the dataset that we fitted our learning algorithm on, as if they were new samples we want to have glucose predicted on. Do you see how the real value and the predicted one differ? The closer we can get them to be the better model we will have!\n\n\n**STOP HERE AND MAKE SURE YOU HAVE UNDERSTOOD THIS CONCEPT**\n\nDo these plots help understand?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n ggplot(DataSmaller, aes(x = age, y = glucose)) +\n    geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency\n    geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) + \n    labs(\n      x = \"Age (years)\",\n      y = \"Glucose Level (mmol/L)\") +\n    theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n      plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n      axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n      panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n      panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n    ) \n```\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n    geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency\n    geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for\n    geom_point(aes(x = age, y = predicted_glucose), size = 2, color = \"grey\", alpha = 0.8) + \n    labs(\n      x = \"Age (years)\",\n      y = \"Glucose Level (mmol/L)\") +\n    theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n      plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n      axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n      panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n      panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n    ) \n```\n\n::: {.cell-output-display}\n![](SupervisedLearningFINAL_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n**DISCUSS THIS WITH YOUR PARTNER AND EXPLAIN IT TO EACH OTHER**\n\n------------------------------------------------------------------------\n\nNow lets fit the decision tree algorithm in an even smaller set of points:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataEvenSmaller <- DataSmaller[1:30, ]\ndim(DataEvenSmaller)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 30  3\n```\n\n\n:::\n:::\n\n\n-   \n\n    1)  What is the model created?\n\n-   \n\n    2)  How does it differ from the previous?\n\n\n::: {.cell}\n\n:::\n\n\n```         \n3)  Do the same with the linear regression model. What changes?\n```\n",
    "supporting": [
      "SupervisedLearningFINAL_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
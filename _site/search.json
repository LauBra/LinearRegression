[
  {
    "objectID": "GradientDescent2.html",
    "href": "GradientDescent2.html",
    "title": "",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(plot3D)"
  },
  {
    "objectID": "GradientDescent2.html#the-gradient-descent-formula",
    "href": "GradientDescent2.html#the-gradient-descent-formula",
    "title": "",
    "section": "The Gradient Descent Formula",
    "text": "The Gradient Descent Formula\nThe general update formula for gradient descent is:\n\\[\nx_{n+1} = x_n - \\alpha f'(x_n)\n\\]\nwhere:\n\n\n\\(x_n\\) is the current guess\n\n\\(f'(x_n)\\) is the gradient at \\(x_n\\)\n\n\n\\(\\alpha\\) is the learning rate (a small positive constant that controls the step size)\n\nThe idea is that the steeper the slope, the larger the update, moving us closer to the minimum."
  },
  {
    "objectID": "GradientDescent2.html#single-variable-gradient-descent",
    "href": "GradientDescent2.html#single-variable-gradient-descent",
    "title": "",
    "section": "Single-Variable Gradient Descent",
    "text": "Single-Variable Gradient Descent\nLet’s replicate the slides seen in class and minimize a single-variable function, \\(f(x) = x^2\\). Imagine we start with an initial guess that the minimum is at \\(x = -4\\) (although we know this is not the true minimum, from class we have seen this happens at \\(x = 0\\)). We will iteratively improve this guess by calculating the derivative (gradient) and adjusting the guess.\nUnderstanding the Derivative’s Role\nIn gradient descent, the derivative (or gradient) tells us the slope of the function at any given point. Here’s the intuition:\n\nIf the derivative is positive: The function is sloping upwards, so we should move “downhill” by decreasing our guess.\nIf the derivative is negative: The function is sloping downwards, so we should move “downhill” by increasing our guess.\n\nThis approach ensures that we move towards the minimum at each step.\nExample: Minimizing \\(f(x) = x^2\\)\n\nUsing function \\(f(x) = x^2\\), first we have to calculate the derivative of this function \\(f'(x) = 2x\\)\n\n#Define the function and its derivative \n\nf &lt;- function(x) {\n  \n  x^2\n}\n\nf_prime &lt;- function(x) {\n  \n  2* x\n  \n}\n\nThis is how they look:\n\nlibrary(ggplot2)\n# Create a sequence of x values\nx_vals &lt;- seq(-10, 10, length.out = 100)\n\n# Compute y values for both functions\ndata &lt;- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n\n           x       f_x f_prime_x\n1 -10.000000 100.00000 -20.00000\n2  -9.797980  96.00041 -19.59596\n3  -9.595960  92.08244 -19.19192\n4  -9.393939  88.24610 -18.78788\n5  -9.191919  84.49138 -18.38384\n6  -8.989899  80.81828 -17.97980\n\nlibrary(patchwork)\n\nA &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x, color = \"f(x) = x^2\"), size = 1) +\n  #geom_line(aes(y = f_prime_x, color = \"f'(x) = 2x\"), size = 1) +\n  labs(title = \"Plot of f(x) = x^2 \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c(\"f(x) = x^2\" = \"blue\", \"f'(x) = 2x\" = \"red\")) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nB &lt;- ggplot(data, aes(x = x)) +\n  #geom_line(aes(y = f_x, color = \"f(x) = x^2\"), size = 1) +\n  geom_line(aes(y = f_prime_x, color = \"f'(x) = 2x\"), size = 1) +\n  labs(title = \"Plot of  f'(x) = 2x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 100) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c(\"f(x) = x^2\" = \"blue\", \"f'(x) = 2x\" = \"red\")) +\n  theme_minimal()\n\nA + B\n\n\n\n\n\n# Set initial values\n\nalpha &lt;- 0.8 # Learning rate \nx &lt;- -10 # Initial guess\nx_store &lt;- NULL\n\nAs previosuly explained, the general update formula for gradient descent is:\n\\[\nx_{n+1} = x_n - \\alpha f'(x_n)\n\\]\nAs you can see, this code iteratively adjusts our guess \\(x\\) based on the gradient at each step, converging towards the minimum. To find the minimum of a function, gradient descent updates the current guess x_n by a step that depends on the gradient (slope) at x_n.\n\n# Gradient descent loop\n\nfor (i in 1:10) { \n  \n  gradient &lt;- f_prime(x) \n  \n  \n  # Update parameters\n  x &lt;- x - alpha * gradient\n\n  # Store values for plotting\n  x_store[i] &lt;- x\n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n  }\n\nStep 1 : x = 6 f(x) = 36 \nStep 2 : x = -3.6 f(x) = 12.96 \nStep 3 : x = 2.16 f(x) = 4.6656 \nStep 4 : x = -1.296 f(x) = 1.679616 \nStep 5 : x = 0.7776 f(x) = 0.6046618 \nStep 6 : x = -0.46656 f(x) = 0.2176782 \nStep 7 : x = 0.279936 f(x) = 0.07836416 \nStep 8 : x = -0.1679616 f(x) = 0.0282111 \nStep 9 : x = 0.100777 f(x) = 0.010156 \nStep 10 : x = -0.06046618 f(x) = 0.003656158 \n\n\nThis code iteratively adjusts \\(x\\) based on the gradient at each step, converging towards the minimum of f(x).\nPlotting it all together\n\nApproximations &lt;- data.frame(x = x_store, y = f(x_store), dy = f_prime(x_store)) %&gt;%\n  add_rownames()# Assign a \"darkening\" group\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\nApproximations$rowname &lt;- as.numeric(Approximations$rowname )\n\nA &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^2 \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nB &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_prime_x),  color = \"blue\", size = 1) +\n  labs(title = \"Plot of  f'(x) = 2x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 100) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = dy, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nA + B\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA larger learning rate \\(\\alpha\\) can make the steps too large, causing the algorithm to “overshoot” the minimum, while a smaller learning rate may result in slow convergence."
  },
  {
    "objectID": "GradientDescent2.html#your-turn",
    "href": "GradientDescent2.html#your-turn",
    "title": "",
    "section": "Your turn!",
    "text": "Your turn!\nUse gradient descent to minimize the following functions. For each function, start with different initial guesses and observe how the algorithm converges to a minimum.\n\\[\nf(x) = x^2 + 3x + 5\n\\]\n\n# Define the function and its derivative \n\nf &lt;- function(x) {\n  x^2 + 3*x + 5\n  \n}\n  \nf_prime &lt;- function(x) {\n  \n  2*x + 3\n  \n}\n\nWhy not plot it?\n\n# Create a sequence of x values\nx_vals &lt;- seq(-15, 15, length.out = 100)\n\n# Compute y values for both functions\ndata &lt;- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n\n          x      f_x f_prime_x\n1 -15.00000 185.0000 -27.00000\n2 -14.69697 176.9100 -26.39394\n3 -14.39394 169.0037 -25.78788\n4 -14.09091 161.2810 -25.18182\n5 -13.78788 153.7420 -24.57576\n6 -13.48485 146.3866 -23.96970\n\nlibrary(patchwork)\n\nA &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x, color = \"f(x) =  x^2 + 3*x + 5\"), size = 1) +\n  #geom_line(aes(y = f_prime_x, color = \"f'(x) = 2x + 1\"), size = 1) +\n  labs(title = \"Plot of f(x) =  x^2 + 3*x + 5\",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c(\"f(x) =  x^2 + 3*x + 5\" = \"blue\")) +\n  theme_minimal()\n\nB &lt;- ggplot(data, aes(x = x)) +\n  #geom_line(aes(y = f_x, color = \"f(x) = x^2\"), size = 1) +\n  geom_line(aes(y = f_prime_x, color = \"f'(x) = 2*x + 3\"), size = 1) +\n  labs(title = \"Plot of  f'(x) = 2*x + 3\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-30, 100) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c( \"f'(x) = 2*x + 3\" = \"red\")) +\n  theme_minimal()\n\nA + B\n\n\n\n\n\n# Parameters\n\nalpha &lt;- 0.01 \nx &lt;- 2 # Initial guess\n\n\n# Gradient descent loop\n\nfor (i in 1:10) { \n  \n  x &lt;- x - alpha * f_prime(x) \n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n  }\n\nStep 1 : x = 1.93 f(x) = 14.5149 \nStep 2 : x = 1.8614 f(x) = 14.04901 \nStep 3 : x = 1.794172 f(x) = 13.60157 \nStep 4 : x = 1.728289 f(x) = 13.17185 \nStep 5 : x = 1.663723 f(x) = 12.75914 \nStep 6 : x = 1.600448 f(x) = 12.36278 \nStep 7 : x = 1.538439 f(x) = 11.98211 \nStep 8 : x = 1.477671 f(x) = 11.61652 \nStep 9 : x = 1.418117 f(x) = 11.26541 \nStep 10 : x = 1.359755 f(x) = 10.9282"
  },
  {
    "objectID": "GradientDescent2.html#part-3-local-vs-global-minima",
    "href": "GradientDescent2.html#part-3-local-vs-global-minima",
    "title": "",
    "section": "Part 3: Local vs Global Minima",
    "text": "Part 3: Local vs Global Minima\nGradient descent may not always reach the global minimum, especially if the function has multiple minima. The algorithm might “get stuck” in a local minimum, particularly if the initial guess is close to one of these minimum.\nConsider the function \\(f(x) = x^4 + x^3 - 2x^2\\), which has both local and global minima.\n\nf &lt;- function(x) {\n  \n  x^4 + x^3 - 2*(x^2) \n}\n  \nf_prime &lt;- function(x){\n  \n  4*(x^3) + 3*(x^2) - 4*x\n}\n\nParameters\n\nalpha &lt;- 0.01 \nx &lt;- 1\nx_store &lt;- NULL\n\n\nfor (i in 1:10) { \n  \n  \n  x &lt;- x - alpha *f_prime(x) \n  \n  x_store[i] &lt;- x\n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n}\n\nStep 1 : x = 0.97 f(x) = -0.08383419 \nStep 2 : x = 0.9440661 f(x) = -0.1467667 \nStep 3 : x = 0.9214345 f(x) = -0.1948753 \nStep 4 : x = 0.9015272 f(x) = -0.2322205 \nStep 5 : x = 0.8838971 f(x) = -0.2615931 \nStep 6 : x = 0.8681921 f(x) = -0.2849583 \nStep 7 : x = 0.8541308 f(x) = -0.303729 \nStep 8 : x = 0.841485 f(x) = -0.3189397 \nStep 9 : x = 0.8300673 f(x) = -0.3313601 \nStep 10 : x = 0.8197226 f(x) = -0.3415714 \n\n\n\nApproximations &lt;- data.frame(x = x_store, y = f(x_store), dy = f_prime(x_store)) %&gt;%\n  add_rownames()# Assign a \"darkening\" group)\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\nApproximations$rowname &lt;- as.numeric(Approximations$rowname )\n\n# Create a sequence of x values\nx_vals &lt;- seq(-15, 15, length.out = 100)\n\n# Compute y values for both functions\ndata &lt;- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n\n          x      f_x  f_prime_x\n1 -15.00000 46800.00 -12765.000\n2 -14.69697 43049.84 -11991.445\n3 -14.39394 39529.24 -11249.729\n4 -14.09091 36228.67 -10539.185\n5 -13.78788 33138.78  -9859.144\n6 -13.48485 30250.42  -9208.938\n\nA_big &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n\n# Create a sequence of x values\nx_vals &lt;- seq(-5, 5, length.out = 100)\n\n# Compute y values for both functions\ndata &lt;- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n\n          x      f_x f_prime_x\n1 -5.000000 450.0000 -405.0000\n2 -4.898990 410.4284 -378.7088\n3 -4.797980 373.4548 -353.5559\n4 -4.696970 338.9655 -329.5168\n5 -4.595960 306.8491 -306.5666\n6 -4.494949 276.9970 -284.6806\n\nA &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n   ylim(-5, 5) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nB &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_prime_x),  color = \"blue\", size = 1) +\n  labs(title = \"Plot of  f'(x) = 4*x^3 + 3*x^2 - 4*x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 90) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = dy, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n(A_big | A) \n\nWarning: Removed 61 rows containing missing values (`geom_line()`).\n\n\n\n\n(A | B )\n\nWarning: Removed 61 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 51 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWe see this function has 2 minimums! One is a local minimim and the other the global minimum. Depending on where we start (intialization point) we will end up in one or the other.\nExercise! Try different initial guesses, record which initial guess leads to global minimum.\nParameters\n\nalpha &lt;- 0.03\nx &lt;- -0.5\nx_store &lt;- NULL\nnumber_iterations &lt;- 100\n\n\nfor (i in 1:number_iterations) { \n  \n  \n  x &lt;- x - alpha *f_prime(x) \n  \n  x_store[i] &lt;- x\n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n}\n\nStep 1 : x = -0.5675 f(x) = -0.7231592 \nStep 2 : x = -0.642653 f(x) = -0.920852 \nStep 3 : x = -0.7250915 f(x) = -1.156317 \nStep 4 : x = -0.813674 f(x) = -1.424506 \nStep 5 : x = -0.9062562 f(x) = -1.712375 \nStep 6 : x = -0.9996069 f(x) = -1.998821 \nStep 7 : x = -1.08963 f(x) = -2.258633 \nStep 8 : x = -1.171997 f(x) = -2.470269 \nStep 9 : x = -1.243079 f(x) = -2.62357 \nStep 10 : x = -1.300817 f(x) = -2.722108 \nStep 11 : x = -1.345069 f(x) = -2.778691 \nStep 12 : x = -1.377285 f(x) = -2.808136 \nStep 13 : x = -1.39977 f(x) = -2.822284 \nStep 14 : x = -1.414967 f(x) = -2.828682 \nStep 15 : x = -1.425001 f(x) = -2.831453 \nStep 16 : x = -1.43152 f(x) = -2.832617 \nStep 17 : x = -1.43571 f(x) = -2.833097 \nStep 18 : x = -1.438384 f(x) = -2.833291 \nStep 19 : x = -1.440082 f(x) = -2.83337 \nStep 20 : x = -1.441158 f(x) = -2.833402 \nStep 21 : x = -1.441838 f(x) = -2.833414 \nStep 22 : x = -1.442267 f(x) = -2.833419 \nStep 23 : x = -1.442538 f(x) = -2.833421 \nStep 24 : x = -1.442709 f(x) = -2.833422 \nStep 25 : x = -1.442817 f(x) = -2.833422 \nStep 26 : x = -1.442885 f(x) = -2.833422 \nStep 27 : x = -1.442928 f(x) = -2.833422 \nStep 28 : x = -1.442955 f(x) = -2.833422 \nStep 29 : x = -1.442972 f(x) = -2.833422 \nStep 30 : x = -1.442982 f(x) = -2.833422 \nStep 31 : x = -1.442989 f(x) = -2.833422 \nStep 32 : x = -1.442993 f(x) = -2.833422 \nStep 33 : x = -1.442996 f(x) = -2.833422 \nStep 34 : x = -1.442998 f(x) = -2.833422 \nStep 35 : x = -1.442999 f(x) = -2.833422 \nStep 36 : x = -1.442999 f(x) = -2.833422 \nStep 37 : x = -1.443 f(x) = -2.833422 \nStep 38 : x = -1.443 f(x) = -2.833422 \nStep 39 : x = -1.443 f(x) = -2.833422 \nStep 40 : x = -1.443 f(x) = -2.833422 \nStep 41 : x = -1.443 f(x) = -2.833422 \nStep 42 : x = -1.443 f(x) = -2.833422 \nStep 43 : x = -1.443 f(x) = -2.833422 \nStep 44 : x = -1.443 f(x) = -2.833422 \nStep 45 : x = -1.443 f(x) = -2.833422 \nStep 46 : x = -1.443 f(x) = -2.833422 \nStep 47 : x = -1.443 f(x) = -2.833422 \nStep 48 : x = -1.443 f(x) = -2.833422 \nStep 49 : x = -1.443 f(x) = -2.833422 \nStep 50 : x = -1.443 f(x) = -2.833422 \nStep 51 : x = -1.443 f(x) = -2.833422 \nStep 52 : x = -1.443 f(x) = -2.833422 \nStep 53 : x = -1.443 f(x) = -2.833422 \nStep 54 : x = -1.443 f(x) = -2.833422 \nStep 55 : x = -1.443 f(x) = -2.833422 \nStep 56 : x = -1.443 f(x) = -2.833422 \nStep 57 : x = -1.443 f(x) = -2.833422 \nStep 58 : x = -1.443 f(x) = -2.833422 \nStep 59 : x = -1.443 f(x) = -2.833422 \nStep 60 : x = -1.443 f(x) = -2.833422 \nStep 61 : x = -1.443 f(x) = -2.833422 \nStep 62 : x = -1.443 f(x) = -2.833422 \nStep 63 : x = -1.443 f(x) = -2.833422 \nStep 64 : x = -1.443 f(x) = -2.833422 \nStep 65 : x = -1.443 f(x) = -2.833422 \nStep 66 : x = -1.443 f(x) = -2.833422 \nStep 67 : x = -1.443 f(x) = -2.833422 \nStep 68 : x = -1.443 f(x) = -2.833422 \nStep 69 : x = -1.443 f(x) = -2.833422 \nStep 70 : x = -1.443 f(x) = -2.833422 \nStep 71 : x = -1.443 f(x) = -2.833422 \nStep 72 : x = -1.443 f(x) = -2.833422 \nStep 73 : x = -1.443 f(x) = -2.833422 \nStep 74 : x = -1.443 f(x) = -2.833422 \nStep 75 : x = -1.443 f(x) = -2.833422 \nStep 76 : x = -1.443 f(x) = -2.833422 \nStep 77 : x = -1.443 f(x) = -2.833422 \nStep 78 : x = -1.443 f(x) = -2.833422 \nStep 79 : x = -1.443 f(x) = -2.833422 \nStep 80 : x = -1.443 f(x) = -2.833422 \nStep 81 : x = -1.443 f(x) = -2.833422 \nStep 82 : x = -1.443 f(x) = -2.833422 \nStep 83 : x = -1.443 f(x) = -2.833422 \nStep 84 : x = -1.443 f(x) = -2.833422 \nStep 85 : x = -1.443 f(x) = -2.833422 \nStep 86 : x = -1.443 f(x) = -2.833422 \nStep 87 : x = -1.443 f(x) = -2.833422 \nStep 88 : x = -1.443 f(x) = -2.833422 \nStep 89 : x = -1.443 f(x) = -2.833422 \nStep 90 : x = -1.443 f(x) = -2.833422 \nStep 91 : x = -1.443 f(x) = -2.833422 \nStep 92 : x = -1.443 f(x) = -2.833422 \nStep 93 : x = -1.443 f(x) = -2.833422 \nStep 94 : x = -1.443 f(x) = -2.833422 \nStep 95 : x = -1.443 f(x) = -2.833422 \nStep 96 : x = -1.443 f(x) = -2.833422 \nStep 97 : x = -1.443 f(x) = -2.833422 \nStep 98 : x = -1.443 f(x) = -2.833422 \nStep 99 : x = -1.443 f(x) = -2.833422 \nStep 100 : x = -1.443 f(x) = -2.833422 \n\n\n\nApproximations &lt;- data.frame(x = x_store, y = f(x_store), dy = f_prime(x_store)) %&gt;%\n  add_rownames()# Assign a \"darkening\" group)\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\nApproximations$rowname &lt;- as.numeric(Approximations$rowname )\n\n# Create a sequence of x values\nx_vals &lt;- seq(-15, 15, length.out = 100)\n\n# Compute y values for both functions\ndata &lt;- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n\n          x      f_x  f_prime_x\n1 -15.00000 46800.00 -12765.000\n2 -14.69697 43049.84 -11991.445\n3 -14.39394 39529.24 -11249.729\n4 -14.09091 36228.67 -10539.185\n5 -13.78788 33138.78  -9859.144\n6 -13.48485 30250.42  -9208.938\n\nA_big &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n\n# Create a sequence of x values\nx_vals &lt;- seq(-5, 5, length.out = 100)\n\n# Compute y values for both functions\ndata &lt;- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n\n          x      f_x f_prime_x\n1 -5.000000 450.0000 -405.0000\n2 -4.898990 410.4284 -378.7088\n3 -4.797980 373.4548 -353.5559\n4 -4.696970 338.9655 -329.5168\n5 -4.595960 306.8491 -306.5666\n6 -4.494949 276.9970 -284.6806\n\nA &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n   ylim(-5, 5) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nB &lt;- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_prime_x),  color = \"blue\", size = 1) +\n  labs(title = \"Plot of  f'(x) = 4*x^3 + 3*x^2 - 4*x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 90) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = dy, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n(A_big | A) \n\nWarning: Removed 61 rows containing missing values (`geom_line()`).\n\n\n\n\n(A | B )\n\nWarning: Removed 61 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 51 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWhat happens at initial point x = 0? And if you increase the learning rate a lot? Does it mean it gets to the minimum faster? But which one? What is another parameter you can modify?….. ITERATIONS!\n\n#Try out any other modification!\n\nAs you have seen:\nThe choice of learning rate \\(\\alpha\\) is crucial:\n\nIf \\(\\alpha\\) too large, the algorithm might oscillate and fail to converge.\nIf \\(\\alpha\\) too slow, requiring more iterations.\n\nExample of high learning rate:\nSetting \\(\\alpha = 0.5\\)"
  },
  {
    "objectID": "GradientDescent2.html#part-4-multivariable-gradient-descent",
    "href": "GradientDescent2.html#part-4-multivariable-gradient-descent",
    "title": "",
    "section": "Part 4: Multivariable Gradient Descent",
    "text": "Part 4: Multivariable Gradient Descent\nMultivariable gradient descent is an extension of the single-variable case. Instead of using a single derivative, we calculate the gradient vector, which consists of the partial derivatives of the function with respect to each variable.\nConsider the function:\n\\[\nf(x, y) = xsin(y) + x^2\n\\]\nThe gradient of this function is:\n\\[\n\\nabla f(x, y) = \\left\\langle \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right\\rangle = \\langle \\sin(y) + 2x, x \\cos(y) \\rangle\n\\]\nLet’s apply gradient descent starting from an initial guess of \\(\\vec{x} = (1, 2)\\) with a learning rate of \\(\\alpha = 0.01\\)\n\n# Define the function and its partial derivatives \n\nf &lt;- function(x, y) {\n  x* sin(y) + x^2 \n}\n  \nf_x &lt;- function(x, y) {\n  \n  sin(y) + 2*x \n  \n}\n  \n\nf_y &lt;- function(x, y){\n  \n  x*cos(y)\n\n}\n\nAnother way to do it in R is using the function Deriv()\n\nlibrary(Deriv) \n\n\n# Define the function f(x, y) = x* sin(y) + x^2\n\nf &lt;- function(x, y) { \n  x*sin(y) + x^2 \n  }\n\n# Compute the gradient symbolically (exact expressions)\n\ngrad_f &lt;- Deriv(expression(x*sin(y) + x^2), c(\"x\", \"y\"))\n\nprint(grad_f)\n\nexpression(c(x = 2 * x + sin(y), y = x * cos(y)))\n\n#expression(c(x = 2 * x + sin(y), y = x * cos(y))) Computes the partial derivatives for you!!!!!\n\ngradient &lt;- function(x, y) { \n  \n  eval(grad_f) \n  }\n\nAs you can see is the partial derivatives above written, directly calculated\n\n# Initial parameters\n\nalpha &lt;- 0.01 # Learning rate \niterations &lt;- 100 # Number of iterations \nx &lt;- 1 # Initial guess for x \ny &lt;- 2 # Initial guess for y\n\n# Store results for plotting\n\nresults &lt;- data.frame(Iteration = 0, x = x, y = y, f_value = f(x, y))\n\n# Gradient descent loop\n\n\nfor (i in 1:iterations) { \n  \n  grad &lt;- gradient(x = x, y = y)\n  \n  # Evaluate gradient \n  x &lt;- x - alpha* grad[1] \n  y &lt;- y - alpha * grad[2] \n  results &lt;- rbind(results, data.frame(Iteration = i, x = x, y = y, f_value = f(x, y))) \n  \n # would be the same as:\n  #x &lt;- x - alpha*f_x(x, y) \n  #y &lt;- y - alpha* f_y(x, y) \n  \n  \n  }\n\n# Display first few iterations\n\nhead(results)\n\n   Iteration         x        y  f_value\n1          0 1.0000000 2.000000 1.909297\nx          1 0.9709070 2.004161 1.823815\nx1         2 0.9424133 2.008239 1.741817\nx2         3 0.9145067 2.012231 1.663164\nx3         4 0.8871751 2.016138 1.587723\nx4         5 0.8604070 2.019960 1.515364\n\n\n\n#Generate grid data for 3D surface plot \n\nx_vals &lt;- seq(-2, 2, length.out = 50) \ny_vals &lt;- seq(-1, 3, length.out = 50) \nz_vals &lt;- outer(x_vals, y_vals, Vectorize(f)) #evaluate x and y values in function f\n\n# 3D plot\n\npersp3D(x = x_vals, y = y_vals, z = z_vals, col = \"lightblue\", theta = 30, phi = 20, expand = 0.6, shade = 0.5, main = \"Gradient Descent Path on f(x, y) = x*sin(y) + x^2\", xlab = \"x\", ylab = \"y\", zlab = \"f(x, y)\")\n\n# Overlay gradient descent path\n\npoints3D(results$x, results$y, results$f_value, col = \"red\", pch = 19, add = TRUE)\nlines3D(results$x, results$y, results$f_value, col = \"red\", add = TRUE)\n\n\n\n\nAs expected, the value of the function is being minimized at each iteration!\n\n# Plot the value of f(x, y) over iterations \n\nggplot(results, aes(x = Iteration, y = f_value)) + geom_line(color = \"blue\") + labs( title = \"Convergence of Gradient Descent on f(x, y) = x*sin(y) + x^2\", x = \"Iteration\", y = \"f(x, y)\" ) + theme_minimal()"
  },
  {
    "objectID": "GradientDescent2.html#final-remarks",
    "href": "GradientDescent2.html#final-remarks",
    "title": "",
    "section": "Final Remarks",
    "text": "Final Remarks\nGradient descent is a versatile optimization technique, but it’s not without limitations:\n\nIt may converge to local minima rather than the global minimum. It is sensitive to the choice of learning rate and initial guess.\nVariants of gradient descent, like stochastic gradient descent (SGD) and momentum-based methods, are often used to address these issues in large-scale machine learning tasks. Understanding and experimenting with gradient descent is crucial for developing an intuition about optimization in machine learning and algorithms."
  },
  {
    "objectID": "LossFunction_Final.html",
    "href": "LossFunction_Final.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "LossFunction_Final.html#define-best-fit-minimizing-error",
    "href": "LossFunction_Final.html#define-best-fit-minimizing-error",
    "title": "",
    "section": "Define “Best Fit” – Minimizing Error",
    "text": "Define “Best Fit” – Minimizing Error\nThe “best fit” line minimizes the average distance (error) between the predicted values and the actual data points. This error (or residual) for a single data point is calculated as:\n\\[\nResidual = ActualValue - Predicted Value\n\\]"
  },
  {
    "objectID": "LossFunction_Final.html#example-with-a-single-data-point",
    "href": "LossFunction_Final.html#example-with-a-single-data-point",
    "title": "",
    "section": "Example with a Single Data Point",
    "text": "Example with a Single Data Point\nLet’s calculate the residual for a single data point in the dataset.\n\nDataSmaller &lt;- Data[1:80,]\n# Calcualte error of line (y = 112 + 0.5x)\nDataSmaller$line1 &lt;- 120 + 0.5 * seq(0, 60, length.out = 80)\nDataSmaller$linePredicted &lt;- 120 + 0.5 *DataSmaller$age\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nDataSmaller$linePredicted &lt;- 120 + 0.5 *DataSmaller$age\n# Get the predicted value for the first data point\npredicted_value &lt;- DataSmaller$linePredicted[63]\n\n# Calculate the residual\nactual_value &lt;- DataSmaller$glucose[63]\nresidual &lt;- actual_value - predicted_value\n\n# Print results\ncat(\"Actual Value:\", actual_value, \"\\n\")\n\nActual Value: 44 \n\ncat(\"Predicted Value:\", predicted_value, \"\\n\")\n\nPredicted Value: 138 \n\ncat(\"Residual (Error):\", residual, \"\\n\")\n\nResidual (Error): -94 \n\n\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n   geom_point(aes(x = DataSmaller$age[63], y = actual_value), size = 3, color = \"darkred\", alpha = 0.7) +\n   geom_point(aes(x = DataSmaller$age[63], y = predicted_value), size = 3, color = \"orange\", alpha = 0.7) +\n  geom_segment(aes(x = DataSmaller$age[63], y = actual_value, xend = DataSmaller$age[63], yend = predicted_value),\n               color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)"
  },
  {
    "objectID": "LossFunction_Final.html#sum-of-errors-squares-or-absolute-value",
    "href": "LossFunction_Final.html#sum-of-errors-squares-or-absolute-value",
    "title": "",
    "section": "Sum of Errors – squares or absolute value?",
    "text": "Sum of Errors – squares or absolute value?\nErrors can be both positive and negative, so simply summing them would lead to cancellation, which isn’t meaningful. Two common methods are:\n\n1.Sum of Squared Residuals: This squares each error, avoiding cancellation and penalizing larger errors more.\n2.Absolute Values of Residuals: Taking absolute values also prevents cancellation, but doesn’t penalize large errors as heavily as squaring."
  },
  {
    "objectID": "LossFunction_Final.html#different-error-metrics",
    "href": "LossFunction_Final.html#different-error-metrics",
    "title": "",
    "section": "Different Error Metrics",
    "text": "Different Error Metrics\nLet’s calculate different error metrics\n\n# Calculate Residuals\nresiduals &lt;- DataSmaller$glucose - DataSmaller$linePredicted\n\n# Define a function to calculate error metrics\ncalculate_errors &lt;- function(residuals) {\n  \n  # Sum of Squared Residuals (SSR)\n  SSR &lt;- sum(residuals^2)\n  \n  # Mean Squared Error (MSE) – equivalent to L2 loss\n  MSE &lt;- (SSR)/dim(DataSmaller)[1]\n  \n  # Root Mean Squared Error (RMSE)\n  RMSE &lt;- sqrt(MSE)\n  \n  # Mean Absolute Error (MAE) – equivalent to L1 loss\n  MAE &lt;- mean(abs(residuals))\n  \n  # Print the results\n  cat(\"Sum of Squared Residuals (SSR):\", SSR, \"\\n\")\n  cat(\"Mean Squared Error (MSE):\", MSE, \"\\n\")\n  cat(\"Mean Absolute Error (MAE):\", MAE, \"\\n\")\n  \n  # Return a list of the error metrics\n  return(list(SSR = SSR, MSE = MSE, RMSE = RMSE, MAE = MAE))\n}\n\nerror_metrics &lt;- calculate_errors(residuals)\n\nSum of Squared Residuals (SSR): 111807 \nMean Squared Error (MSE): 1397.588 \nMean Absolute Error (MAE): 30.575"
  },
  {
    "objectID": "LossFunction_Final.html#choosing-a-loss",
    "href": "LossFunction_Final.html#choosing-a-loss",
    "title": "",
    "section": "Choosing a loss",
    "text": "Choosing a loss\nDeciding whether to use MAE or MSE can depend on the dataset and the way you want to handle certain predictions. Most feature values in a dataset typically fall within a distinct range.Values outside the typical range and would be considered an outlier.\nWhen choosing the best loss function, consider how you want the model to treat outliers. For instance, MSE moves the model more toward the outliers, while MAE doesn’t. L2 loss incurs a much higher penalty for an outlier than L1 loss.\n\n\nMSE vs MAE\n\n\nRegardless, the functions that we will use that implement linear regression algorithms (e.g lm()) take into account MSE error, so this will not be part of any decision we have to take. The reason for this is that MSE has benefits that MAE has not in terms of optimizimg it! We will learn about this later.\nOutliers\nIn data pre-processing we discussed outliers, and here we will try and visually understand their influence when modeling.\n\n# Fit a linear model\nmodel_MSE &lt;- lm(glucose ~ age, data = DataSmaller)\nDataSmaller$predictions_MSE &lt;- predict(model_MSE, DataSmaller)\n\nprint(model_MSE$coefficients)\n\n(Intercept)         age \n  73.677460    1.330072 \n\n\nCalculate residuals\n\nresiduals_MSE &lt;- DataSmaller$glucose - DataSmaller$predictions_MSE\nresiduals_MSE &lt;- model_MSE$residuals # can also extract them directly from the model!\n\nCalculate all losses\n\nerror_metrics_MSE &lt;- calculate_errors(residuals_MSE)\n\nSum of Squared Residuals (SSR): 81632.69 \nMean Squared Error (MSE): 1020.409 \nMean Absolute Error (MAE): 24.23672 \n\n\nPlot linear regression model\n\nMSE &lt;- ggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE)[1], slope = coef(model_MSE)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE without Outliers\", x = \"Age\", y = \"Glucose\")\n\nAdd Outliers\n\n# Introduce outliers \n\nDataOutliers &lt;- DataSmaller\nDataOutliers$glucose[c(1, 3, 5)] &lt;- DataOutliers$glucose[c(1, 3, 5)] * 3 # Changing 3 readings into 3 times their value! \n\nmodel_MSE_out &lt;- lm(glucose ~ age, data = DataOutliers)\nDataOutliers$predictions_MSE_out &lt;- predict(model_MSE_out, DataOutliers)\n\n#Calculate residuals\n\nresiduals_MSE_out &lt;- DataOutliers$glucose - DataOutliers$predictions_MSE_out\nresiduals_MSE_out &lt;- model_MSE_out$residuals # can also extract them directly from the model!\n\n#Calculate loss \n\nerror_metrics_MSE_out &lt;- calculate_errors(residuals_MSE_out)\n\nSum of Squared Residuals (SSR): 430818.6 \nMean Squared Error (MSE): 5385.232 \nMean Absolute Error (MAE): 38.6216 \n\nMSE_Out &lt;- ggplot(DataOutliers, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE_out)[1], slope = coef(model_MSE_out)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE Outliers\", x = \"Age\", y = \"Glucose\")\n\n\nlibrary(patchwork)\n\nMSE + MSE_Out \n\n\n\n\n\nAs key learning points: MAE vs MSE?\n\n\nLook at the scales! - MAE is more interpretable as it gives a more straightforward interpretation of the “average error,” as it represents the median prediction error.\nRobustness to Outliers: MAE is less sensitive to outliers than MSE because it doesn’t square the errors. But the functions used to implement linear regression use mostly MSE, because MAE is not differentiable, so it requires specialised optimisation algorithms, which are less computationally efficient than least-squares (MSE) for large datasets as MSE is differentiable and so easier to optimize. (we will learn about what this means later!)"
  },
  {
    "objectID": "MSE_Change_FINAL.html",
    "href": "MSE_Change_FINAL.html",
    "title": "(3) Best Fit Line",
    "section": "",
    "text": "library(mlbench)\nlibrary(tidyverse) \nlibrary(ggplot2)\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n\nLoad diabetes dataset (already available by installing package mlbench). This is a toy dataset that has been extensively used in many machine learning examples\n\ndata(\"PimaIndiansDiabetes\")\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\nLets now select only two of this columns age and glucose and store it as a new dataframe\n\nData &lt;- PimaIndiansDiabetes %&gt;%\n        select(age, glucose)\n\nLets implement this loss function in R and test it with our diabetes data.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plot3D)\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(patchwork)\n\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n\nLoad diabetes dataset, lets make the dataset even smaller so we can properly understand\n\ndata(\"PimaIndiansDiabetes\")\n\nData &lt;- PimaIndiansDiabetes %&gt;%\n  select(age, glucose, mass) %&gt;%\n  add_rownames(var = \"Patient ID\")\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\nDataSmaller &lt;- Data[1:3,]\n\nx &lt;- DataSmaller$age\ny &lt;- DataSmaller$glucose\n\nSo we have narrowed down that we are continuing exploring MSE loss function to identify the line of best fit. MSE in the end can also be written as:\n\\[\n\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n\\] Because the predicted y comes out of this model! So they mean the same thing.\nPAUSE for understanding!\nThis can be coded like this:\n\nmse_practical &lt;- function(beta0, beta1, x, y, m) {\n  (1 / m) * sum((y - (beta0 + beta1 * x))^2)\n}\n\nPut it to test with beta values B0 = 3 and B1 = 5 , then try with B0 = 4 and B1 = 7. Do it in a piece of paper by hand too. Remember, we already have the y (outcome label - glucose), x (glucose) and m (number of samples) defined.\n\nage &lt;- DataSmaller$age\nglucose &lt;- DataSmaller$glucose\nDataPoints &lt;- length(x)\n\nmse_practical(3, 5, age, glucose, DataPoints)\n\n[1] 5584.667\n\n\nRemember this is the same as:\n\nmse_practical(beta0 = 3, beta1 = 5, x = age, y= glucose, m =DataPoints)\n\n[1] 5584.667\n\n\nNow calculate it by hand! Use R as help (big numbers we are dealing with, but make sure you get the concept)\n Repeat process with B0 = 4 and B1 = 7. Figure continued here:\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n\n[1] 5584.667\n\n\nSame as above!\nNow do the same for B0 = 4 and B1 = 7.\n\nClick here to reveal the solution\n\nmse_practical(beta0 = 4, beta1 = 7, x = age, y= glucose, m =DataPoints)\n\n[1] 20985.67\n\n\nMSE is lower with parameters beta0 = 3, beta1 = 5, but is it the lowest it can go? Lets plot!\nAs you can see in this plot we have two lines with the slopes previously selected. The one with the lower loss fits the data better.\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point() +\n  geom_abline(slope = 3, intercept = 5, color = \"blue\", size = 1) +\n  geom_abline(slope = 4, intercept = 7, color = \"red\", size = 1) +\n  xlim(0, 80) +\n  ylim(0, 300) +\n  labs(title = \"Different betas\",\n       x = \"Age\", y = \"Glucose\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nWe could try this out for a range of different parameters. But, very slow.\nFor example lets go back to more data points:\n\nDataSmaller &lt;- Data[1:80,]\n\nB0_values &lt;- seq(60, 80, by = 10)\nB1_values &lt;- seq(0.5, 2, by = 0.5)\n\nx_range &lt;- seq(min(DataSmaller$age), max(DataSmaller$age), length.out = 100)\n\n\nline_data &lt;- expand.grid(B0 = B0_values, B1 = B1_values) %&gt;%\n  group_by(B0, B1) %&gt;%\n  do(data.frame(x = x_range, y = .$B0 + .$B1 * x_range)) %&gt;%\n  ungroup() %&gt;%\n  mutate(label = paste(\"B0 =\", B0, \", B1 =\", B1))\n# Plot the original data points with the different regression lines\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"black\", size = 3) +  # Original data points\n  geom_line(data = line_data, aes(x = x, y = y, color = label, linetype = label), size = 1) +\n  labs(title = \"Effect of Different B0 and B1 Values on the Regression Line\",\n       x = \"Age (Predictor)\", y = \"Glucose (Response)\",\n       color = \"Parameter Combination\", linetype = \"Parameter Combination\") +\n  theme_minimal()\n\n\n\n\nWe cannot just keep trying randomly! How does the lm() function find out the parameters corresponding to the lowest MSE?\n\nSo we need a way in which to find the combination of parameters that yields the minimum MSE value (i.e the chosen loss function) and so the linear regression model that best fits the data points. But what is the minimum of the loss function?\nLets plot the function for us to understand it better! As we have three parameters that vary, to study the function, we need a 3D plot.\n\\[\nMSE(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n\\]\nUsing same betas as above but more spaced out:\n\n#B0_values &lt;- seq(10, 60, by = 20)\n#B1_values &lt;- seq(0.5, 8, by = 2)\n\nB0_values &lt;- seq(20, 120, by = 0.5)\nB1_values &lt;- seq(-5, 10, by = 0.1)\n\nWe can calculate the MSE for each combination (same way as we did by hand before) and store it in a matrix\n\ndf &lt;- expand_grid(B0_values, B1_values ) %&gt;% \n  rowwise() %&gt;%\n  mutate(MSE = mse_practical(beta0 = B0_values, beta1 = B1_values, x = DataSmaller$age, y = DataSmaller$glucose, m=length(DataSmaller$glucose)))\n\nhead(df)\n\n# A tibble: 6 × 3\n# Rowwise: \n  B0_values B1_values    MSE\n      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1        20      -5   82650.\n2        20      -4.9 80554.\n3        20      -4.8 78485.\n4        20      -4.7 76444.\n5        20      -4.6 74430.\n6        20      -4.5 72443.\n\n\nNow lets plot this function!\n\n\n\n\n\n\nWhat is the minimum MSE? This one! Roughly we can see that it corresponds to around x. What we also find from here is the idea that the minim point is the one at the bottom of the 3D curved surface.\n\nlibrary(plotly)\n\nmin(df$MSE)\n\n[1] 1020.578\n\n# Find the minimum point\nfilter(df, MSE == min(df$MSE))\n\n# A tibble: 1 × 3\n# Rowwise: \n  B0_values B1_values   MSE\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      74.5       1.3 1021.\n\n\n\nfig &lt;- plot_ly(\n  x = B1_values, \n  y = B0_values, \n  z = MSE_matrix,  # Use the reshaped matrix\n  type = \"surface\"\n) %&gt;%\n  layout(\n    title = \"3D Surface Plot of MSE\",\n    scene = list(\n      xaxis = list(title = \"B1_values\"),\n      yaxis = list(title = \"B0_values\"),\n      zaxis = list(title = \"MSE\"), \n       camera = list(\n        eye = list(x = 2, y = 2, z = 1) # Adjust these values for the angle\n      )\n    )\n  )\n\n# Add the minimum point\nfig &lt;- fig %&gt;%\n  add_trace(\n    x = filter(df, MSE == min(df$MSE))$B1_values,\n    y = filter(df, MSE == min(df$MSE))$B0_values,\n    z = filter(df, MSE == min(df$MSE))$MSE,\n    mode = \"markers\",\n    type = \"scatter3d\",\n    marker = list(size = 10, color = \"red\"),\n    name = \"Minimum MSE\"\n  )\n\n# Render the plot\nfig\n\n\n\n\n\nThese parameters are very similar to what we obtained from the lm() function (and would be the same if we had sampled the MSE at that accurate decimal point)\n\nlm(glucose ~ age, data = DataSmaller)\n\n\nCall:\nlm(formula = glucose ~ age, data = DataSmaller)\n\nCoefficients:\n(Intercept)          age  \n      73.68         1.33  \n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "OLS_Derivation.html",
    "href": "OLS_Derivation.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "OLS_Derivation.html#setting-up-the-minimization-problem",
    "href": "OLS_Derivation.html#setting-up-the-minimization-problem",
    "title": "",
    "section": "Setting Up the Minimization Problem",
    "text": "Setting Up the Minimization Problem\nWe start with the following minimization objective:\n\\[\n\\min_{\\hat{\\beta_0}, \\hat{\\beta_1}} \\sum_{i=1}^{N} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\n\\tag{1}\\]\nThis problem is solved by taking partial derivatives with respect to \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) and setting them to zero.\nStep 1\n\n\nTake the partial derivative with respect to \\(\\hat{\\beta_0}\\):\n\\[\n\\frac{\\partial W}{\\partial \\hat{\\beta_0}} = \\sum_{i=1}^{N} -2(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0\n\\tag{2}\\]\n\n\nTake the partial derivative with respect to \\(\\hat{\\beta_1}\\):\n\\[\n\\frac{\\partial W}{\\partial \\hat{\\beta_1}} = \\sum_{i=1}^{N} -2 x_i (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0\n\\tag{3}\\]\n\n\nLet \\(W = \\sum_{i=1}^{N} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\\). Now our task is to solve these equations using algebra."
  },
  {
    "objectID": "OLS_Derivation.html#solving-for-hatbeta_0",
    "href": "OLS_Derivation.html#solving-for-hatbeta_0",
    "title": "",
    "section": "Solving for \\(\\hat{\\beta_0}\\)\n",
    "text": "Solving for \\(\\hat{\\beta_0}\\)\n\nFollowing from eq.2 we can drop the \\(-2\\) (this is why sometimes you see a loss function with a \\(\\frac{1}{2}\\) at the start):\n\\[\n\\sum_{i=1}^{N} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0\n\\]\nExpanding and rearranging terms (using:\\(\\quad \\sum_{i=1}^{N} y_i = N \\bar{y}\\)), we find:\n\\[\nN \\hat{\\beta_0} = \\sum_{i=1}^{N} y_i - \\hat{\\beta_1} \\sum_{i=1}^{N} x_i\n\\]\nDividing by \\(N\\) gives us:\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\n\\]\nwhere \\(\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\) and \\(\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N} x_i\\)."
  },
  {
    "objectID": "OLS_Derivation.html#solving-for-hatbeta_1",
    "href": "OLS_Derivation.html#solving-for-hatbeta_1",
    "title": "",
    "section": "Solving for \\(\\hat{\\beta_1}\\)\n",
    "text": "Solving for \\(\\hat{\\beta_1}\\)\n\nTo solve for \\(\\hat{\\beta_1}\\), remove the \\(-2\\) and rearrange to get \\(\\sum_{i=1}^{N} x_i y_i - \\hat{\\beta_0} x_i - \\hat{\\beta_1} x_i^2\\) then substitute \\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\\). This yields:\n\\[\n\\sum_{i=1}^{N} x_i y_i - (\\bar{y} -\\hat{\\beta_1}\\bar{x})x_i -  \\hat{\\beta_1} x_i^2 = 0\n\\]\nAs the summation is applying to everything in th3 above equation, we can distribute the sum to each term (and pull constant terms out in front of the summation) getting:\n\\[\n\\sum_{i=1}^{N} x_i y_i - \\bar{y}\\sum_{i=1}^{N}x_i + \\hat{\\beta_1}\\bar{x} \\sum_{i=1}^{N} x_i - \\hat{\\beta_1} \\sum_{i=1}^{N} x_i^2 = 0\n\\]\nUsing again \\(\\quad \\sum_{i=1}^{N} y_i = N \\bar{y}\\), we solve for \\(\\hat{\\beta_1}\\) and get\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}\n\\]\nFrom here, we then apply \\(\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\\)\n\n\n\n\n\n\nDerivation\n\n\n\nStep 1: Expand the Left-Hand Side\n\\[\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\n\\]\nWe start with the left-hand side:\n\\[\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nExpanding this product:\n\\[\n= \\sum_{i=1}^{N} \\left( x_i y_i - x_i \\bar{y} - \\bar{x} y_i + \\bar{x} \\bar{y} \\right)\n\\]\nStep 2: Separate the Summation\nNow, we can separate each term inside the summation:\n\\[\n= \\sum_{i=1}^{N} x_i y_i - \\sum_{i=1}^{N} x_i \\bar{y} - \\sum_{i=1}^{N} \\bar{x} y_i + \\sum_{i=1}^{N} \\bar{x} \\bar{y}\n\\]\nStep 3: Simplify Each Term\nLet’s simplify each of these four terms:\n\nFirst Term: \\(\\sum_{i=1}^{N} x_i y_i\\) remains as it is.\n\nSecond Term: Since \\(\\bar{y}\\) is a constant (the mean of \\(y\\)), we can factor it out of the summation:\n\\[\n\\sum_{i=1}^{N} x_i \\bar{y} = \\bar{y} \\sum_{i=1}^{N} x_i\n\\]\n\nThird Term: Similarly, since \\(\\bar{x}\\) is constant, we can factor it out of the summation:\n\n\\[\n   \\sum_{i=1}^{N} \\bar{x} y_i = \\bar{x} \\sum_{i=1}^{N} y_i\n\\]\n\n\nFourth Term: Since both \\(\\bar{x}\\) and \\(\\bar{y}\\) are constants, we can factor them both out, giving:\n\\[\n\\sum_{i=1}^{N} \\bar{x} \\bar{y} = N \\bar{x} \\bar{y}\n\\]\n\nStep 4: Substitute Back and Simplify\nSubstitute each of these simplified terms back into the original expression:\n\\[\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - \\bar{y} \\sum_{i=1}^{N} x_i - \\bar{x} \\sum_{i=1}^{N} y_i + N \\bar{x} \\bar{y}\n\\]\nStep 5: Substitute\nRecall that \\(\\bar{x} = \\frac{1}{N} \\sum\\*{i=1}\\^{N} x_i\\) and \\(\\* \\bar{y} = \\frac{1}{N} \\sum{i=1}\\^{N} y_i\\). Thus:\n\\[\n\\sum_{i=1}^{N} x_i = N \\bar{x} \\quad \\text{and} \\quad \\sum_{i=1}^{N} y_i = N \\bar{y}\n\\]\nSubstitute these into the expression:\n\\[\n= \\sum_{i=1}^{N} x_i y_i - \\bar{y} (N \\bar{x}) - \\bar{x} (N \\bar{y}) + N \\bar{x} \\bar{y}\n\\]\nStep 6: Combine Terms\nNotice that the terms \\(- \\bar{y} (N \\bar{x})\\) and \\(- \\bar{x} (N \\bar{y})\\) are both equal to \\(- N \\bar{x} \\bar{y}\\), which cancels with the \\(+ N \\bar{x} \\bar{y}\\) term:\n\\[\n= \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\n\\]\nFinal Result\nWe have derived:\n\\[\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\n\\]\n\n\n\nThe final OLS estimates are:\n\n\nSlope \\(\\hat{\\beta_1}\\):\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2} = \\frac{Cov(x,y)}{Var(x)}\n\\]\n\n\nIntercept \\(\\hat{\\beta_0}\\):\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\n\\]\n\n\n\n@https://are.berkeley.edu/courses/EEP118/current/derive_ols.pdf"
  },
  {
    "objectID": "SupervisedLearning.html",
    "href": "SupervisedLearning.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "SupervisedLearning.html#a-supervised-learning-algorithm-linear-regression",
    "href": "SupervisedLearning.html#a-supervised-learning-algorithm-linear-regression",
    "title": "",
    "section": "A Supervised Learning Algorithm: Linear Regression",
    "text": "A Supervised Learning Algorithm: Linear Regression\nSuppose we want to predict our glucose level based on our age with this dataset. We could fit a learning algorithm to our dataset and create a predictive model. In this case the learning algorithm is going to be linear regression. ggplot already contains the functionality of fitting and visualizing this linear regression model through geom_smooth as seen below:\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_smooth(method='lm', se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThis amounts to using the lm function to fit the data as you have seen in the Statistics Module statistics recap\nTo recap, the linear regression algorithm is as follows:\nIn ML, we write the equation as\n\\[\ny = \\beta_0 + \\beta_1\\, x_1\n\\]\nwhere\n\ny = outcome label ( column to predict )\n_0 = sometimes known as bias, it is the intercept of the line and is a parameter of the model\n_1 = weight of the feature/column/x 1 - same as slope in equation of a line (if we only had 2 dimensions) and is a parameter of the model\nx_1 = feature/column 1/ input data\n\nAnd how do we find out the two paramters? They are the result of fitting the learning algorithm (linear regression) to the data. Once we have them defined, we can then say we have the predictive algorithm where, if we were to have a new sample (\\(x_i\\)) with age information, we could find out the predicted glucose (\\(y_i'\\)).\nAs you will see later, te way this fitting works, is by finding the parameters _0 and _1 which make the best fit line.And this is calculated through optimization of the loss/cost function through either ordinary least squares (OLS)  or gradient descent (do not worry about this just now!)\nMore specifically, when you use the function lm below or any other function that fits a linear model in R or Python, _0 and _1 or _n (depending on the amount of features you are including in your model) are being calculated using OLS\n\n#ls_fit &lt;- lm(y ~ x, data = data)`\nls_fit &lt;- lm(glucose ~ age, data = DataSmaller)\n\nDisplay a summary of fit\n\nsummary(ls_fit)\n\n\nCall:\nlm(formula = glucose ~ age, data = DataSmaller)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-102.94  -16.49   -3.56   19.69   73.07 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  73.6775    11.9684   6.156 3.02e-08 ***\nage           1.3301     0.3237   4.110 9.71e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.35 on 78 degrees of freedom\nMultiple R-squared:  0.178, Adjusted R-squared:  0.1674 \nF-statistic: 16.89 on 1 and 78 DF,  p-value: 9.715e-05\n\n\n\ncoef(ls_fit)\n\n(Intercept)         age \n  73.677460    1.330072 \n\n\nIf we include this coefficients and draw the line ourselves, we will obtain the same thing as the geom_smooth above.\n\n#| fig-align: 'center'\n#| fig-width: 6\n#| fig-height: 4\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe predictive model then (with fiited parameters) is:\n\\[\nglucose = 73.68 + 1.33*age\n\\] If we now have a new sample coming with \\(age = 45\\), what is the predicted glucose? Can you include it in the plot?\n\nage_newsample&lt;- 45\npredicted_glucose &lt;-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n\n[1] 133.53\n\n\nThis is the same as doing:\n\npredict(ls_fit, newdata = data.frame(age = 45)) # we had to create a new dataframe with one value of age for this to work, as it normally expects more than 1 smple, and more features!\n\n       1 \n133.5307 \n\n\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n\n\n\n\nQuestion!! If my new sample is \\(age = 10\\), can I still predict glucose, even though none of my data used to create the model had a sample with age = 10?\n\nage_newsample&lt;- 10\npredicted_glucose &lt;-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n\n[1] 86.98\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) + \n  xlim(0, 100)\n\n\n\n\nYes we can! Another thing is how confident we are about the predicted value, but we will discuss that later on.The model is an infinite line, mapping age to glucose (so we could even map negative values, or very high age values (e.g 1000) and obtain a predicted glucose - but it would make no sense).\n\n\n\n\n# EXERCISE: Understand the role of the dataset in predictive modelling\n\n\nLets fit the decision tree learning algorithm in different number of training points (you will learn more about how decision trees work next Monday, but as you have seen in the previous classes, learning algorithms are already coded for you as functions, so you can apply them already)\n\n\n::: {.cell}\n\n\n```{.r .cell-code} library(rpart) #has functions for decision tree algorithm library(rpart.plot) library(ggplot2)\n\n\n#Decision tree algorithm tree_model &lt;- rpart(glucose ~ age, data = DataSmaller, method = “anova”)\n\n\n# Plot the decision tree rpart.plot(tree_model, type = 3, fallen.leaves = TRUE, box.palette = “Blues”, shadow.col = “gray”) ```\n\n\n::: {.cell-output-display}  ::: :::\n\n\n::: {.cell}\n\n\n{.r .cell-code} # Print the model for further details print(tree_model)\n\n\n::: {.cell-output .cell-output-stdout} ``` n= 80\n\n\nnode), split, n, deviance, yval * denotes terminal node\n\n\n1) root 80 99307.69 120.5625 2) age&lt; 22.5 10 8122.90 78.9000  3) age&gt;=22.5 70 71347.49 126.5143 6) age&lt; 38.5 42 34265.90 117.9524 12) age&gt;=26.5 31 22403.87 114.9355  13) age&lt; 26.5 11 10784.73 126.4545  7) age&gt;=38.5 28 29384.43 139.3571 14) age&lt; 50.5 16 16919.75 132.6250  15) age&gt;=50.5 12 10772.67 148.3333 * ``` ::: :::\n\n\n::: {.cell}\n\n\n\n{.r .cell-code} #Get the predicted glucose values DataSmaller$predicted_glucose &lt;- predict(tree_model, newdata = DataSmaller) :::\n\n\nThis we are now introducing the dataset that we fitted our learning algorithm on, as if they were new samples we want to have glucose predicted on. Do you see how the real value and the predicted one differ? The closer we can get them to be the better model we will have!\n\n\n::: {.cell}\n\n\n\n{.r .cell-code} View(DataSmaller) :::\n\n\nSTOP HERE AND MAKE SURE YOU HAVE UNDERSTOOD THIS CONCEPT\n\n\nDo these plots help understand?\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(DataSmaller, aes(x = age, y = glucose)) + geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for #geom_step(aes(y = predicted_glucose), colour = \"darkred\",  size = 2) +  # labs( x = \"Age (years)\", y = \"Glucose Level (mmol/L)\") + theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size theme( plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title plot.subtitle = element_text(hjust = 0.5),              # Center subtitle axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity )\n\n\n::: {.cell-output-display}  ::: :::\n\n\n::: {.cell}\n\n\n{.r .cell-code} ggplot(DataSmaller, aes(x = age, y = glucose)) + geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for geom_point(aes(x = age, y = predicted_glucose), size = 2, color = \"grey\", alpha = 0.8) + labs( x = \"Age (years)\", y = \"Glucose Level (mmol/L)\") + theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size theme( plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title plot.subtitle = element_text(hjust = 0.5),              # Center subtitle axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity )\n\n\n::: {.cell-output-display}  ::: :::\n\n\nDISCUSS THIS WITH YOUR PARTNER AND EXPLAIN IT TO EACH OTHER\n\n\n\nNow lets fit the decision tree algorithm in an even smaller set of points:\n\nDataEvenSmaller &lt;- DataSmaller[1:30, ]\ndim(DataEvenSmaller)\n\n[1] 30  3\n\n\n\n\nWhat is the model created?\n\n\nHow does it differ from the previous?\n\n\n\n\nDo the same with the linear regression model. What changes?"
  },
  {
    "objectID": "SupervisedLearningFINAL.html",
    "href": "SupervisedLearningFINAL.html",
    "title": "(1) Supervised Learning Models",
    "section": "",
    "text": "Mention that you can just copy the code and insert it as an Rmd!\nSupervised learning models can make predictions after seeing lots of data with the correct answers and then discovering the connections between the elements in the data that produce the correct answers. This is like a student learning new material by studying old exams that contain both questions and answers. Once the student has trained on enough old exams, the student is well prepared to take a new exam. These ML systems are “supervised” in the sense that a human gives the ML system data with the known correct results\nPackages needed for this session:\nlibrary(mlbench) #where PimaIndiansDiabetes database is found\nlibrary(tidyverse) #for minimal data wrangling \nlibrary(ggplot2) #already part of tidyverse - so a bit redundant\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\nLoad diabetes dataset (already available by installing package mlbench). This is a toy dataset that has been extensively used in many machine learning examples\ndata(\"PimaIndiansDiabetes\")\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\nLets now select only two of this columns age and glucose and store it as a new dataframe\nData &lt;- PimaIndiansDiabetes %&gt;%\n        select(age, glucose)\nRecall this is the same thing as the below code, but using the library tidyverse’s data wrangling options. For more learning check the exercise from data pre-processing class: https://rintrouob.quarto.pub/hds-msc---module-3---preprocessingeda/HealthyR_Summary.html\nData &lt;- PimaIndiansDiabetes[, c(\"age\", \"glucose\")]\nWe have 768 observations/rows, so lets cut it down to just 30, for the sake of easier visualization and take a look\nDataSmaller &lt;- Data[1:80,]\n\nhead(DataSmaller)\n\n  age glucose\n1  50     148\n2  31      85\n3  32     183\n4  21      89\n5  33     137\n6  30     116\nNow lets visualize this relationship\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\")",
    "crumbs": [
      "(1) Supervised Learning Models"
    ]
  },
  {
    "objectID": "SupervisedLearningFINAL.html#a-supervised-learning-algorithm-linear-regression",
    "href": "SupervisedLearningFINAL.html#a-supervised-learning-algorithm-linear-regression",
    "title": "(1) Supervised Learning Models",
    "section": "A Supervised Learning Algorithm: Linear Regression",
    "text": "A Supervised Learning Algorithm: Linear Regression\nSuppose we want to predict our glucose level based on our age with this dataset. We could fit a learning algorithm to our dataset and create a predictive model. In this case the learning algorithm is going to be linear regression. ggplot already contains the functionality of fitting and visualizing this linear regression model through geom_smooth as seen below:\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_smooth(method='lm', se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThis amounts to using the lm function to fit the data as you have seen in the Statistics Module statistics recap\nTo recap, the linear regression algorithm is as follows:\nIn ML, we write the equation as\n\\[\ny = \\beta_0 + \\beta_1\\, x_1\n\\]\nwhere\n\ny = outcome label ( column to predict )\n_0 = sometimes known as bias, it is the intercept of the line and is a parameter of the model\n_1 = weight of the feature/column/x 1 - same as slope in equation of a line (if we only had 2 dimensions) and is a parameter of the model\nx_1 = feature/column 1/ input data\n\nAnd how do we find out the two paramters? They are the result of fitting the learning algorithm (linear regression) to the data. Once we have them defined, we can then say we have the predictive algorithm where, if we were to have a new sample (\\(x_i\\)) with age information, we could find out the predicted glucose (\\(y_i'\\)).\nAs you will see later, te way this fitting works, is by finding the parameters _0 and _1 which make the best fit line.And this is calculated through optimization of the loss/cost function through either ordinary least squares (OLS) or gradient descent (do not worry about this just now!)\nMore specifically, when you use the function lm below or any other function that fits a linear model in R or Python, _0 and _1 or _n (depending on the amount of features you are including in your model) are being calculated using OLS\n\n#ls_fit &lt;- lm(y ~ x, data = data)`\nls_fit &lt;- lm(glucose ~ age, data = DataSmaller)\n\nDisplay a summary of fit\n\nsummary(ls_fit)\n\n\nCall:\nlm(formula = glucose ~ age, data = DataSmaller)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-102.94  -16.49   -3.56   19.69   73.07 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  73.6775    11.9684   6.156 3.02e-08 ***\nage           1.3301     0.3237   4.110 9.71e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.35 on 78 degrees of freedom\nMultiple R-squared:  0.178, Adjusted R-squared:  0.1674 \nF-statistic: 16.89 on 1 and 78 DF,  p-value: 9.715e-05\n\n\n\ncoef(ls_fit)\n\n(Intercept)         age \n  73.677460    1.330072 \n\n\nIf we include this coefficients and draw the line ourselves, we will obtain the same thing as the geom_smooth above.\n\n#| fig-align: 'center'\n#| fig-width: 6\n#| fig-height: 4\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nThe predictive model then (with fiited parameters) is:\n\\[\nglucose = 73.68 + 1.33*age\n\\] If we now have a new sample coming with \\(age = 45\\), what is the predicted glucose? Can you include it in the plot?\n\nage_newsample&lt;- 45\npredicted_glucose &lt;-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n\n[1] 133.53\n\n\nThis is the same as doing:\n\npredict(ls_fit, newdata = data.frame(age = 45)) # we had to create a new dataframe with one value of age for this to work, as it normally expects more than 1 smple, and more features!\n\n       1 \n133.5307 \n\n\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5)\n\n\n\n\n\n\n\nQuestion!! If my new sample is \\(age = 10\\), can I still predict glucose, even though none of my data used to create the model had a sample with age = 10?\n\nage_newsample&lt;- 10\npredicted_glucose &lt;-  73.68 + 1.33*age_newsample\nprint(predicted_glucose)\n\n[1] 86.98\n\nggplot(DataSmaller, aes(x = age, y = glucose)) + \ngeom_point() + \ngeom_point(x = age_newsample,y = predicted_glucose, colour = \"grey\", size = 5 ) +\nlabs(x = \"Age (Feature)\", y = \"Glucose (Label)\") +  \ngeom_abline(intercept = coef(ls_fit)[1], slope = coef(ls_fit)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) + \n  xlim(0, 100)\n\n\n\n\n\n\n\nYes we can! Another thing is how confident we are about the predicted value, but we will discuss that later on.The model is an infinite line, mapping age to glucose (so we could even map negative values, or very high age values (e.g 1000) and obtain a predicted glucose - but it would make no sense).\nEXERCISE: Understand the role of the dataset in predictive modelling\nLets fit the decision tree learning algorithm in different number of training points (you will learn more about how decision trees work next Monday, but as you have seen in the previous classes, learning algorithms are already coded for you as functions, so you can apply them already)\n\nlibrary(rpart) #has functions for decision tree algorithm\nlibrary(rpart.plot)\nlibrary(ggplot2)\n\n#Decision tree algorithm\ntree_model &lt;- rpart(glucose ~ age, data = DataSmaller, method = \"anova\")\n\n# Plot the decision tree\nrpart.plot(tree_model, type = 3, fallen.leaves = TRUE, box.palette = \"Blues\", shadow.col = \"gray\")\n\n\n\n\n\n\n\n\n# Print the model for further details\nprint(tree_model)\n\nn= 80 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 80 99307.69 120.5625  \n   2) age&lt; 22.5 10  8122.90  78.9000 *\n   3) age&gt;=22.5 70 71347.49 126.5143  \n     6) age&lt; 38.5 42 34265.90 117.9524  \n      12) age&gt;=26.5 31 22403.87 114.9355 *\n      13) age&lt; 26.5 11 10784.73 126.4545 *\n     7) age&gt;=38.5 28 29384.43 139.3571  \n      14) age&lt; 50.5 16 16919.75 132.6250 *\n      15) age&gt;=50.5 12 10772.67 148.3333 *\n\n\n\n#Get the predicted glucose values \nDataSmaller$predicted_glucose &lt;- predict(tree_model, newdata = DataSmaller)\n\nThis we are now introducing the dataset that we fitted our learning algorithm on, as if they were new samples we want to have glucose predicted on. Do you see how the real value and the predicted one differ? The closer we can get them to be the better model we will have!\n\nView(DataSmaller)\n\nSTOP HERE AND MAKE SURE YOU HAVE UNDERSTOOD THIS CONCEPT\nDo these plots help understand?\n\n ggplot(DataSmaller, aes(x = age, y = glucose)) +\n    geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency\n geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for\n    #geom_step(aes(y = predicted_glucose), colour = \"darkred\",  size = 2) +  # \n    labs(\n      x = \"Age (years)\",\n      y = \"Glucose Level (mmol/L)\") +\n    theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n      plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n      axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n      panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n      panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n    ) \n\n\n\n\n\n\n\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n    geom_point(size = 3, color = \"blue\", alpha = 0.7) +  # Larger, colored points with some transparency\n    geom_step( aes(y = predicted_glucose), colour = \"darkred\", size = 2) +  # Step function for\n    geom_point(aes(x = age, y = predicted_glucose), size = 2, color = \"grey\", alpha = 0.8) + \n    labs(\n      x = \"Age (years)\",\n      y = \"Glucose Level (mmol/L)\") +\n    theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n      plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n      axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n      panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n      panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n    ) \n\n\n\n\n\n\n\nDISCUSS THIS WITH YOUR PARTNER AND EXPLAIN IT TO EACH OTHER\n\nNow lets fit the decision tree algorithm in an even smaller set of points:\n\nDataEvenSmaller &lt;- DataSmaller[1:30, ]\ndim(DataEvenSmaller)\n\n[1] 30  3\n\n\n\n\nWhat is the model created?\n\n\nHow does it differ from the previous?\n\n\n3)  Do the same with the linear regression model. What changes?",
    "crumbs": [
      "(1) Supervised Learning Models"
    ]
  },
  {
    "objectID": "DecisionThreshold.html",
    "href": "DecisionThreshold.html",
    "title": "2 Decision threshold",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# Load the Pima Indians Diabetes dataset\ndata(\"PimaIndiansDiabetes\")\n\n\n# Narrow down the dataset\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes)\n\n# Convert the outcome variable to binary (0 = neg, 1 = pos)\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(diabetes_dummy = ifelse(diabetes == \"pos\", 1, 0))\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# Predict probabilities\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(predicted_probability = predict(logistic_model, type = \"response\"))\n\n\n# Create classifications for different thresholds\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(\n    class_0_5 = ifelse(predicted_probability &gt;= 0.5, 1, 0),\n    class_0_3 = ifelse(predicted_probability &gt;= 0.3, 1, 0),\n    class_0_7 = ifelse(predicted_probability &gt;= 0.7, 1, 0)\n  )\n\nWhat is hapenning when we increase the threshold? See how the number of correct (true y and predicted y) change!\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_3)\n\n   \n      0   1\n  0 331 169\n  1  66 202\n\n\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_5)\n\n   \n      0   1\n  0 443  57\n  1 138 130\n\n\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_7)\n\n   \n      0   1\n  0 484  16\n  1 195  73\n\n\n\n\n\n\n\n Back to top"
  }
]
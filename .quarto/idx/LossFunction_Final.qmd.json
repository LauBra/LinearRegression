{"title":"(2) Loss functions","markdown":{"headingText":"(2) Loss functions","containsRefs":false,"markdown":"\n```{r}\n#| message: false\n\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(ggplot2)\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n\nLoad diabetes dataset (already available by installing package [mlbench](https://mlbench.github.io)). This is a toy dataset that has been extensively used in many [machine learning examples](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/code)\n\n```{r}\ndata(\"PimaIndiansDiabetes\")\n```\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\n\nLets now select only two of this columns `age` and `glucose` and store it as a new dataframe\n\n```{r}\nData <- PimaIndiansDiabetes %>%\n        select(age, glucose)\n```\n\nRecall this is the same thing as\n\n```{r}\nData <- PimaIndiansDiabetes[, c(\"age\", \"glucose\")]\n```\n\nWe have *768* observations/rows, so lets cut it down to just 30, for the sake of easier visualization and take a look\n\n```{r}\nDataSmaller <- Data[1:80,]\n\nhead(DataSmaller)\n\n```\n\n\n## Define \"Best Fit\" – Minimizing Error\n\nThe *\"best fit\"* line minimizes the average distance (error) between the predicted values and the actual data points. This error (or residual) for a single data point is calculated as:\n\n$$\nResidual = ActualValue - Predicted Value\n$$\n\n## Example with a Single Data Point\n\nLet's calculate the residual for a single data point in the dataset.\n\n```{r}\n\nDataSmaller <- Data[1:80,]\n# Calcualte error of line (y = 112 + 0.5x)\nDataSmaller$line1 <- 120 + 0.5 * seq(0, 60, length.out = 80)\nDataSmaller$linePredicted <- 120 + 0.5 *DataSmaller$age\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)\n\n```\n\n\n```{r}\n\nDataSmaller$linePredicted <- 120 + 0.5 *DataSmaller$age\n# Get the predicted value for the first data point\npredicted_value <- DataSmaller$linePredicted[63]\n\n# Calculate the residual\nactual_value <- DataSmaller$glucose[63]\nresidual <- actual_value - predicted_value\n\n# Print results\ncat(\"Actual Value:\", actual_value, \"\\n\")\ncat(\"Predicted Value:\", predicted_value, \"\\n\")\ncat(\"Residual (Error):\", residual, \"\\n\")\n```\n\n```{r}\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(size = 3, color = \"black\", alpha = 0.7) +  # Larger, colored points with some transparency\n   geom_point(aes(x = DataSmaller$age[63], y = actual_value), size = 3, color = \"darkred\", alpha = 0.7) +\n   geom_point(aes(x = DataSmaller$age[63], y = predicted_value), size = 3, color = \"orange\", alpha = 0.7) +\n  geom_segment(aes(x = DataSmaller$age[63], y = actual_value, xend = DataSmaller$age[63], yend = predicted_value),\n               color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Age (years)\",\n    y = \"Glucose Level (mmol/L)\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +  # Horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n  theme_minimal(base_size = 18) +  # Use a clean theme with larger base font size\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),  # Center and bold title\n    plot.subtitle = element_text(hjust = 0.5),              # Center subtitle\n    axis.title = element_text(face = \"bold\"),               # Bold axis titles for readability\n    panel.grid.major = element_line(color = \"grey85\"),      # Lighten grid for subtlety\n    panel.grid.minor = element_blank()                      # Remove minor grid lines for clarity\n  ) +\n  geom_line(aes(y = line1, x = seq(0, 60, length.out = 80)), color = \"darkorange\", size = 1)\n\n```\n\n## Sum of Errors – squares or absolute value?\n\nErrors can be both positive and negative, so simply summing them would lead to cancellation, which isn’t meaningful. Two common methods are:\n\n- __1.Sum of Squared Residuals__: This squares each error, avoiding cancellation and penalizing larger errors more.\n\n- __2.Absolute Values of Residuals__: Taking absolute values also prevents cancellation, but doesn’t penalize large errors as heavily as squaring.\n\n## Different Error Metrics\nLet's calculate different error metrics\n\n```{r}\n# Calculate Residuals\nresiduals <- DataSmaller$glucose - DataSmaller$linePredicted\n\n# Define a function to calculate error metrics\ncalculate_errors <- function(residuals) {\n  \n  # Sum of Squared Residuals (SSR)\n  SSR <- sum(residuals^2)\n  \n  # Mean Squared Error (MSE) – equivalent to L2 loss\n  MSE <- (SSR)/dim(DataSmaller)[1]\n  \n  # Root Mean Squared Error (RMSE)\n  RMSE <- sqrt(MSE)\n  \n  # Mean Absolute Error (MAE) – equivalent to L1 loss\n  MAE <- mean(abs(residuals))\n  \n  # Print the results\n  cat(\"Sum of Squared Residuals (SSR):\", SSR, \"\\n\")\n  cat(\"Mean Squared Error (MSE):\", MSE, \"\\n\")\n  cat(\"Mean Absolute Error (MAE):\", MAE, \"\\n\")\n  \n  # Return a list of the error metrics\n  return(list(SSR = SSR, MSE = MSE, RMSE = RMSE, MAE = MAE))\n}\n\nerror_metrics <- calculate_errors(residuals)\n\n```\n\n## Choosing a loss\n\nDeciding whether to use MAE or MSE can depend on the dataset and the way you want to handle certain predictions. Most feature values in a dataset typically fall within a distinct range.Values outside the typical range and would be considered an __outlier__.\n\nWhen choosing the best loss function, consider how you want the model to treat outliers. For instance, __MSE moves the model more toward the outliers, while MAE doesn't__. L2 loss incurs a much higher penalty for an outlier than L1 loss. \n\n![MSE vs MAE](/Users/bravol/Desktop/ML Class/Practical/LinearReg_Lecture/MSE_MAE.png){fig-align=\"center\"}\n\n\nRegardless, the functions that we will use that implement linear regression algorithms (e.g `lm()`) take into account MSE error, so this will not be part of any decision we have to take. The reason for this is that MSE has benefits that MAE has not in terms of *optimizimg* it! We will learn about this later.  \n\n### Outliers\n\nIn data pre-processing we discussed *outliers*, and here we will try and visually understand their influence when modeling. \n\n```{r}\n# Fit a linear model\nmodel_MSE <- lm(glucose ~ age, data = DataSmaller)\nDataSmaller$predictions_MSE <- predict(model_MSE, DataSmaller)\n\nprint(model_MSE$coefficients)\n```\n\nCalculate residuals\n\n```{r}\n\nresiduals_MSE <- DataSmaller$glucose - DataSmaller$predictions_MSE\nresiduals_MSE <- model_MSE$residuals # can also extract them directly from the model!\n\n```\n\nCalculate all losses \n\n```{r}\nerror_metrics_MSE <- calculate_errors(residuals_MSE)\n```\n\nPlot linear regression model\n```{r}\n\nMSE <- ggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE)[1], slope = coef(model_MSE)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE without Outliers\", x = \"Age\", y = \"Glucose\")\n\n```\n\nAdd Outliers\n\n```{r}\n# Introduce outliers \n\nDataOutliers <- DataSmaller\nDataOutliers$glucose[c(1, 3, 5)] <- DataOutliers$glucose[c(1, 3, 5)] * 3 # Changing 3 readings into 3 times their value! \n\nmodel_MSE_out <- lm(glucose ~ age, data = DataOutliers)\nDataOutliers$predictions_MSE_out <- predict(model_MSE_out, DataOutliers)\n\n#Calculate residuals\n\nresiduals_MSE_out <- DataOutliers$glucose - DataOutliers$predictions_MSE_out\nresiduals_MSE_out <- model_MSE_out$residuals # can also extract them directly from the model!\n\n#Calculate loss \n\nerror_metrics_MSE_out <- calculate_errors(residuals_MSE_out)\n\nMSE_Out <- ggplot(DataOutliers, aes(x = age, y = glucose)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(model_MSE_out)[1], slope = coef(model_MSE_out)[2], color=\"red\",\n               linetype=\"dashed\", size=1.5) +\n  labs(title = \"MSE Outliers\", x = \"Age\", y = \"Glucose\")\n\n\n```\n\n\n\n\n```{r}\nlibrary(patchwork)\n\nMSE + MSE_Out \n```\n\n\n---------\n\n\n\nAs key learning points:  MAE vs MSE?\n\n- *Look at the scales!* - MAE is more interpretable as it gives a more straightforward interpretation of the \"average error,\" as it represents the median prediction error.\n- Robustness to Outliers: MAE is less sensitive to outliers than MSE because it doesn't square the errors.\nBut the functions used to implement linear regression use mostly MSE, because MAE is not differentiable, so it requires specialised optimisation algorithms, which are less computationally efficient than least-squares (MSE) for large datasets as MSE is differentiable and so easier to optimize. (we will learn about what this means later!)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"python":{"version":"r-reticulate"},"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LossFunction_Final.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
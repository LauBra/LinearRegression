{
  "hash": "e68e90afe821d80172e4774038628f48",
  "result": {
    "engine": "knitr",
    "markdown": "# (5) Derivation of OLS Estimator\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n:::\n\n\nThe Ordinary Least Squares (OLS) estimator is derived by solving a minimization problem. Our objective is to find estimates for the intercept and slope coefficients, denoted $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$, that minimize the sum of squared residuals.\n\n## Setting Up the Minimization Problem\n\nWe start with the following minimization objective:\n\n$$\n\\min_{\\hat{\\beta_0}, \\hat{\\beta_1}} \\sum_{i=1}^{N} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\n$$ {#eq-1}\n\nThis problem is solved by taking partial derivatives with respect to $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ and setting them to zero.\n\n### Step 1\n\n1.  Take the partial derivative with respect to $\\hat{\\beta_0}$:\n\n    $$\n    \\frac{\\partial W}{\\partial \\hat{\\beta_0}} = \\sum_{i=1}^{N} -2(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0\n    $$ {#eq-2}\n\n2.  Take the partial derivative with respect to $\\hat{\\beta_1}$:\n\n    $$\n    \\frac{\\partial W}{\\partial \\hat{\\beta_1}} = \\sum_{i=1}^{N} -2 x_i (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0\n    $$ {#eq-3}\n\nLet $W = \\sum_{i=1}^{N} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2$. Now our task is to solve these equations using algebra.\n\n## Solving for $\\hat{\\beta_0}$\n\nFollowing from eq.2 we can drop the $-2$ (this is why sometimes you see a loss function with a $\\frac{1}{2}$ at the start):\n\n$$\n\\sum_{i=1}^{N} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i) = 0\n$$\n\nExpanding and rearranging terms (using:$\\quad \\sum_{i=1}^{N} y_i = N \\bar{y}$), we find:\n\n$$\nN \\hat{\\beta_0} = \\sum_{i=1}^{N} y_i - \\hat{\\beta_1} \\sum_{i=1}^{N} x_i\n$$\n\nDividing by $N$ gives us:\n\n$$\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\n$$\n\nwhere $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$ and $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N} x_i$.\n\n## Solving for $\\hat{\\beta_1}$\n\nTo solve for $\\hat{\\beta_1}$, remove the $-2$ and rearrange to get $\\sum_{i=1}^{N} x_i y_i -  \\hat{\\beta_0}  x_i - \\hat{\\beta_1}  x_i^2$ then substitute $\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}$. This yields:\n\n$$\n\\sum_{i=1}^{N} x_i y_i - (\\bar{y} -\\hat{\\beta_1}\\bar{x})x_i -  \\hat{\\beta_1} x_i^2 = 0\n$$\n\nAs the summation is applying to everything in th3 above equation, we can distribute the sum to each term (and pull constant terms out in front of the summation) getting:\n\n$$\n\\sum_{i=1}^{N} x_i y_i - \\bar{y}\\sum_{i=1}^{N}x_i + \\hat{\\beta_1}\\bar{x} \\sum_{i=1}^{N} x_i - \\hat{\\beta_1} \\sum_{i=1}^{N} x_i^2 = 0\n$$\n\nUsing again $\\quad \\sum_{i=1}^{N} y_i = N \\bar{y}$, we solve for $\\hat{\\beta_1}$ and get\n\n$$\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}\n$$\n\nFrom here, we then apply $\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}$\n\n::: callout-note\n## Derivation\n\n### Step 1: Expand the Left-Hand Side\n\n$$\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\n$$\n\nWe start with the left-hand side:\n\n$$\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})\n$$\n\nExpanding this product:\n\n$$\n= \\sum_{i=1}^{N} \\left( x_i y_i - x_i \\bar{y} - \\bar{x} y_i + \\bar{x} \\bar{y} \\right)\n$$\n\n### Step 2: Separate the Summation\n\nNow, we can separate each term inside the summation:\n\n$$\n= \\sum_{i=1}^{N} x_i y_i - \\sum_{i=1}^{N} x_i \\bar{y} - \\sum_{i=1}^{N} \\bar{x} y_i + \\sum_{i=1}^{N} \\bar{x} \\bar{y}\n$$\n\n### Step 3: Simplify Each Term\n\nLet's simplify each of these four terms:\n\n1.  **First Term**: $\\sum_{i=1}^{N} x_i y_i$ remains as it is.\n\n2.  **Second Term**: Since $\\bar{y}$ is a constant (the mean of $y$), we can factor it out of the summation:\n\n    $$\n    \\sum_{i=1}^{N} x_i \\bar{y} = \\bar{y} \\sum_{i=1}^{N} x_i\n    $$\n\n3.  **Third Term**: Similarly, since $\\bar{x}$ is constant, we can factor it out of the summation:\n\n$$\n   \\sum_{i=1}^{N} \\bar{x} y_i = \\bar{x} \\sum_{i=1}^{N} y_i\n$$\n\n4.  **Fourth Term**: Since both $\\bar{x}$ and $\\bar{y}$ are constants, we can factor them both out, giving:\n\n    $$\n    \\sum_{i=1}^{N} \\bar{x} \\bar{y} = N \\bar{x} \\bar{y}\n    $$\n\n### Step 4: Substitute Back and Simplify\n\nSubstitute each of these simplified terms back into the original expression:\n\n$$\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - \\bar{y} \\sum_{i=1}^{N} x_i - \\bar{x} \\sum_{i=1}^{N} y_i + N \\bar{x} \\bar{y}\n$$\n\n### Step 5: Substitute\n\nRecall that $\\bar{x} = \\frac{1}{N} \\sum\\*{i=1}\\^{N} x_i$ and $\\* \\bar{y} = \\frac{1}{N} \\sum{i=1}\\^{N} y_i$. Thus:\n\n$$\n\\sum_{i=1}^{N} x_i = N \\bar{x} \\quad \\text{and} \\quad \\sum_{i=1}^{N} y_i = N \\bar{y}\n$$\n\nSubstitute these into the expression:\n\n$$\n= \\sum_{i=1}^{N} x_i y_i - \\bar{y} (N \\bar{x}) - \\bar{x} (N \\bar{y}) + N \\bar{x} \\bar{y}\n$$\n\n### Step 6: Combine Terms\n\nNotice that the terms $- \\bar{y} (N \\bar{x})$ and $- \\bar{x} (N \\bar{y})$ are both equal to $- N \\bar{x} \\bar{y}$, which cancels with the $+ N \\bar{x} \\bar{y}$ term:\n\n$$\n= \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\n$$\n\n### Final Result\n\nWe have derived:\n\n$$\n\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{N} x_i y_i - N \\bar{x} \\bar{y}\n$$\n:::\n\nThe final OLS estimates are:\n\n1.  **Slope** $\\hat{\\beta_1}$:\n\n    $$\n    \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2} = \\frac{Cov(x,y)}{Var(x)}\n    $$\n\n2.  **Intercept** $\\hat{\\beta_0}$:\n\n    $$\n    \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\n    $$\n\n------------------------------------------------------------------------\n\n@https://are.berkeley.edu/courses/EEP118/current/derive_ols.pdf\n\n# Exercises\n\nNow lets compute them in R. Going back to our PIMS glucose dataset we have:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(patchwork)\n\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"PimaIndiansDiabetes\")\n\nData <- PimaIndiansDiabetes %>%\n  select(age, glucose, mass) %>%\n  add_rownames(var = \"Patient ID\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nDataSmaller <- Data[1:80,]\n\nx <- DataSmaller$age\ny <-  DataSmaller$glucose\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb1hat <- cov(x, y) / var(x)\nb1hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.330072\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0hat = mean(y) - b1hat * mean(x)\nb0hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 73.67746\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- linear_reg() %>% \n            set_engine('lm') %>% # adds lm implementation of linear regression\n            set_mode('regression')\n\n\nfit1 <- lm_model %>% \n          fit(y ~ x, data.frame(x,y))\n\n#fit1 <- lm(y~x, data.frame(x,y))\n# verifying the OLS solution\n#summary(fit1)$coeff\n```\n:::\n\n\nThey agree exactly! Excellent. So our *lm()* is really doing OLS regression.\n\nSince R does all the calculations for you, it’s not necessary to know how to derive the OLS solutions (especially with more than one independent variable X), but it is handy to know the intuition behind it, especially when we get to more complicated regression.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
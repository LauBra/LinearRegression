{"title":"(3) Best Fit Line","markdown":{"yaml":{"title":"(3) Best Fit Line","format":"html"},"headingText":"Plot the original data points with the different regression lines","containsRefs":false,"markdown":"\n\n\n```{r}\n#| message: false\n\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(ggplot2)\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n\nLoad diabetes dataset (already available by installing package [mlbench](https://mlbench.github.io)). This is a toy dataset that has been extensively used in many [machine learning examples](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/code)\n\n```{r}\ndata(\"PimaIndiansDiabetes\")\n```\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\n\nLets now select only two of this columns `age` and `glucose` and store it as a new dataframe\n\n```{r}\nData <- PimaIndiansDiabetes %>%\n        select(age, glucose)\n```\n\n\nLets implement this loss function in R and test it with our diabetes data.\n\n```{r}\n#| message: false\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plot3D)\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(patchwork)\n\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n\nLoad diabetes dataset, lets make the dataset even smaller so we can properly understand\n\n```{r}\n\n\ndata(\"PimaIndiansDiabetes\")\n\nData <- PimaIndiansDiabetes %>%\n  select(age, glucose, mass) %>%\n  add_rownames(var = \"Patient ID\")\n\nDataSmaller <- Data[1:3,]\n\nx <- DataSmaller$age\ny <- DataSmaller$glucose\n\n\n```\n\nSo we have narrowed down that we are continuing exploring MSE loss function to identify the line of best fit.  MSE in the end can also be written as: \n\n\n$$\n\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n$$\nBecause the predicted y comes out of this model! So they mean the same thing. \n\n*PAUSE for understanding!*\n\nThis can be coded like this: \n\n```{r}\nmse_practical <- function(beta0, beta1, x, y, m) {\n  (1 / m) * sum((y - (beta0 + beta1 * x))^2)\n}\n```\n\nPut it to test with beta values B0 = 3 and B1 = 5 , then try with B0 = 4 and B1 = 7. Do it in a piece of paper by hand too. Remember, we already have the y (outcome label - glucose), x (glucose) and m (number of samples)  defined.\n\n```{r}\n\nage <- DataSmaller$age\nglucose <- DataSmaller$glucose\nDataPoints <- length(x)\n\nmse_practical(3, 5, age, glucose, DataPoints)\n\n```\n\nRemember this is the same as:\n\n```{r}\n\nmse_practical(beta0 = 3, beta1 = 5, x = age, y= glucose, m =DataPoints)\n\n```\n\nNow calculate it by hand! Use R as help (big numbers we are dealing with, but make sure you get the concept)\n\n![](/Users/bravol/Desktop/ML Class/Practical/LinearReg_Lecture.jpg){fig-align=\"center\"} Repeat process with B0 = 4 and B1 = 7.\nFigure continued here: \n\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n\n```{r}\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n```\n\nSame as above!\n\n\nNow do the same for B0 = 4 and B1 = 7.\n\n<details>\n\n<summary>Click here to reveal the solution</summary>\n\n```{r}\n\nmse_practical(beta0 = 4, beta1 = 7, x = age, y= glucose, m =DataPoints)\n\n```\n\n\n\n</details>\n\nMSE is lower with parameters beta0 = 3, beta1 = 5, but is it the lowest it can go? Lets plot!\n\n\nAs you can see in this plot we have two lines with the slopes previously selected. The one with the lower loss fits the data better.\n\n```{r}\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point() +\n  geom_abline(slope = 3, intercept = 5, color = \"blue\", size = 1) +\n  geom_abline(slope = 4, intercept = 7, color = \"red\", size = 1) +\n  xlim(0, 80) +\n  ylim(0, 300) +\n  labs(title = \"Different betas\",\n       x = \"Age\", y = \"Glucose\") +\n  theme_minimal()\n\n```\n\nWe could try this out for a range of different parameters. But, very slow. \n\nFor example lets go back to more data points:\n\n```{r}\n\nDataSmaller <- Data[1:80,]\n\nB0_values <- seq(60, 80, by = 10)\nB1_values <- seq(0.5, 2, by = 0.5)\n\nx_range <- seq(min(DataSmaller$age), max(DataSmaller$age), length.out = 100)\n\n\nline_data <- expand.grid(B0 = B0_values, B1 = B1_values) %>%\n  group_by(B0, B1) %>%\n  do(data.frame(x = x_range, y = .$B0 + .$B1 * x_range)) %>%\n  ungroup() %>%\n  mutate(label = paste(\"B0 =\", B0, \", B1 =\", B1))\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"black\", size = 3) +  # Original data points\n  geom_line(data = line_data, aes(x = x, y = y, color = label, linetype = label), size = 1) +\n  labs(title = \"Effect of Different B0 and B1 Values on the Regression Line\",\n       x = \"Age (Predictor)\", y = \"Glucose (Response)\",\n       color = \"Parameter Combination\", linetype = \"Parameter Combination\") +\n  theme_minimal()\n\n```\nWe cannot just keep trying randomly! How does the `lm()` function find out the parameters corresponding to the lowest MSE? \n\n--------------------------------------------------\n\n\nSo we need a way in which to find the combination of parameters that yields the minimum MSE value (i.e the chosen loss function) and so the linear regression model that best fits the data points. *But what is the minimum of the loss function?*\n\nLets plot the function for us to understand it better! As we have three parameters that vary, to study the function, we need a 3D plot.\n\n$$\nMSE(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n$$\n\nUsing same betas as above but more spaced out:\n\n```{r}\n#B0_values <- seq(10, 60, by = 20)\n#B1_values <- seq(0.5, 8, by = 2)\n\nB0_values <- seq(20, 120, by = 0.5)\nB1_values <- seq(-5, 10, by = 0.1)\n```\n\nWe can calculate the MSE for each combination (same way as we did by hand before) and store it in a matrix\n\n```{r}\n\ndf <- expand_grid(B0_values, B1_values ) %>% \n  rowwise() %>%\n  mutate(MSE = mse_practical(beta0 = B0_values, beta1 = B1_values, x = DataSmaller$age, y = DataSmaller$glucose, m=length(DataSmaller$glucose)))\n\nhead(df)\n```\n\n\n\nNow lets plot this function!\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(plotly)\nlibrary(htmlwidgets)\n\nwide_df <- df %>%\n  pivot_wider(\n    names_from = B1_values,  # Column names from B1_values\n    values_from = MSE        # Fill these columns with MSE values\n  )\n\n# Convert wide data to matrix for Plotly\nMSE_matrix <- as.matrix(wide_df[, -1])  # Exclude the first column (B0_values)\n\n# Extract B0 and B1 values\nB0_values <- sort(wide_df$B0_values)\nB1_values <- as.numeric(colnames(MSE_matrix))  # Column names become B1 values\n\n\n# If the rows of MSE_matrix need to be flipped, reorder them\n#MSE_matrix <- MSE_matrix[order(B0_values), ]\n\nplot_ly(\n  x = B1_values, \n  y = B0_values, \n  z = MSE_matrix,  # Use the reshaped matrix\n  type = \"surface\"\n) %>%\n  layout(\n    scene = list(\n      xaxis = list(title = \"B1_values\"),\n      yaxis = list(title = \"B0_values\"),\n      zaxis = list(title = \"MSE\")\n    )\n  )\n\n\n```\n\nWhat is the minimum MSE? This one! Roughly we can see that it corresponds to around x. What we also find from here is the idea that the minim point is the one at the bottom of the 3D curved surface.\n\n\n```{r}\nlibrary(plotly)\n\nmin(df$MSE)\n# Find the minimum point\nfilter(df, MSE == min(df$MSE))\n```\n\n\n```{r}\n\nfig <- plot_ly(\n  x = B1_values, \n  y = B0_values, \n  z = MSE_matrix,  # Use the reshaped matrix\n  type = \"surface\"\n) %>%\n  layout(\n    title = \"3D Surface Plot of MSE\",\n    scene = list(\n      xaxis = list(title = \"B1_values\"),\n      yaxis = list(title = \"B0_values\"),\n      zaxis = list(title = \"MSE\"), \n       camera = list(\n        eye = list(x = 2, y = 2, z = 1) # Adjust these values for the angle\n      )\n    )\n  )\n\n# Add the minimum point\nfig <- fig %>%\n  add_trace(\n    x = filter(df, MSE == min(df$MSE))$B1_values,\n    y = filter(df, MSE == min(df$MSE))$B0_values,\n    z = filter(df, MSE == min(df$MSE))$MSE,\n    mode = \"markers\",\n    type = \"scatter3d\",\n    marker = list(size = 10, color = \"red\"),\n    name = \"Minimum MSE\"\n  )\n\n# Render the plot\nfig\n\n```\n\n\n\n\nThese parameters are very similar to what we obtained from the `lm()` function (and would be the same if we had sampled the MSE at that accurate decimal point)\n\n```{r}\n\n\nlm(glucose ~ age, data = DataSmaller)\n\n\n```\n\n","srcMarkdownNoYaml":"\n\n\n```{r}\n#| message: false\n\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(ggplot2)\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n\nLoad diabetes dataset (already available by installing package [mlbench](https://mlbench.github.io)). This is a toy dataset that has been extensively used in many [machine learning examples](https://www.kaggle.com/datasets/mathchi/diabetes-data-set/code)\n\n```{r}\ndata(\"PimaIndiansDiabetes\")\n```\n\nIn the environment now you should see PimaIndiansDiabetes dataframe loaded\n\nLets now select only two of this columns `age` and `glucose` and store it as a new dataframe\n\n```{r}\nData <- PimaIndiansDiabetes %>%\n        select(age, glucose)\n```\n\n\nLets implement this loss function in R and test it with our diabetes data.\n\n```{r}\n#| message: false\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plot3D)\nlibrary(mlbench)\nlibrary(tidyverse) \nlibrary(patchwork)\n\n\ntheme_set(theme_bw()) # to help in plot visualization (white background)\n```\n\nLoad diabetes dataset, lets make the dataset even smaller so we can properly understand\n\n```{r}\n\n\ndata(\"PimaIndiansDiabetes\")\n\nData <- PimaIndiansDiabetes %>%\n  select(age, glucose, mass) %>%\n  add_rownames(var = \"Patient ID\")\n\nDataSmaller <- Data[1:3,]\n\nx <- DataSmaller$age\ny <- DataSmaller$glucose\n\n\n```\n\nSo we have narrowed down that we are continuing exploring MSE loss function to identify the line of best fit.  MSE in the end can also be written as: \n\n\n$$\n\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n$$\nBecause the predicted y comes out of this model! So they mean the same thing. \n\n*PAUSE for understanding!*\n\nThis can be coded like this: \n\n```{r}\nmse_practical <- function(beta0, beta1, x, y, m) {\n  (1 / m) * sum((y - (beta0 + beta1 * x))^2)\n}\n```\n\nPut it to test with beta values B0 = 3 and B1 = 5 , then try with B0 = 4 and B1 = 7. Do it in a piece of paper by hand too. Remember, we already have the y (outcome label - glucose), x (glucose) and m (number of samples)  defined.\n\n```{r}\n\nage <- DataSmaller$age\nglucose <- DataSmaller$glucose\nDataPoints <- length(x)\n\nmse_practical(3, 5, age, glucose, DataPoints)\n\n```\n\nRemember this is the same as:\n\n```{r}\n\nmse_practical(beta0 = 3, beta1 = 5, x = age, y= glucose, m =DataPoints)\n\n```\n\nNow calculate it by hand! Use R as help (big numbers we are dealing with, but make sure you get the concept)\n\n![](/Users/bravol/Desktop/ML Class/Practical/LinearReg_Lecture.jpg){fig-align=\"center\"} Repeat process with B0 = 4 and B1 = 7.\nFigure continued here: \n\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n\n```{r}\n((148 - (253))^2 + (85 - (158))^2 + (183 - (163))^2)/3\n```\n\nSame as above!\n\n\nNow do the same for B0 = 4 and B1 = 7.\n\n<details>\n\n<summary>Click here to reveal the solution</summary>\n\n```{r}\n\nmse_practical(beta0 = 4, beta1 = 7, x = age, y= glucose, m =DataPoints)\n\n```\n\n\n\n</details>\n\nMSE is lower with parameters beta0 = 3, beta1 = 5, but is it the lowest it can go? Lets plot!\n\n\nAs you can see in this plot we have two lines with the slopes previously selected. The one with the lower loss fits the data better.\n\n```{r}\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point() +\n  geom_abline(slope = 3, intercept = 5, color = \"blue\", size = 1) +\n  geom_abline(slope = 4, intercept = 7, color = \"red\", size = 1) +\n  xlim(0, 80) +\n  ylim(0, 300) +\n  labs(title = \"Different betas\",\n       x = \"Age\", y = \"Glucose\") +\n  theme_minimal()\n\n```\n\nWe could try this out for a range of different parameters. But, very slow. \n\nFor example lets go back to more data points:\n\n```{r}\n\nDataSmaller <- Data[1:80,]\n\nB0_values <- seq(60, 80, by = 10)\nB1_values <- seq(0.5, 2, by = 0.5)\n\nx_range <- seq(min(DataSmaller$age), max(DataSmaller$age), length.out = 100)\n\n\nline_data <- expand.grid(B0 = B0_values, B1 = B1_values) %>%\n  group_by(B0, B1) %>%\n  do(data.frame(x = x_range, y = .$B0 + .$B1 * x_range)) %>%\n  ungroup() %>%\n  mutate(label = paste(\"B0 =\", B0, \", B1 =\", B1))\n# Plot the original data points with the different regression lines\n\nggplot(DataSmaller, aes(x = age, y = glucose)) +\n  geom_point(color = \"black\", size = 3) +  # Original data points\n  geom_line(data = line_data, aes(x = x, y = y, color = label, linetype = label), size = 1) +\n  labs(title = \"Effect of Different B0 and B1 Values on the Regression Line\",\n       x = \"Age (Predictor)\", y = \"Glucose (Response)\",\n       color = \"Parameter Combination\", linetype = \"Parameter Combination\") +\n  theme_minimal()\n\n```\nWe cannot just keep trying randomly! How does the `lm()` function find out the parameters corresponding to the lowest MSE? \n\n--------------------------------------------------\n\n\nSo we need a way in which to find the combination of parameters that yields the minimum MSE value (i.e the chosen loss function) and so the linear regression model that best fits the data points. *But what is the minimum of the loss function?*\n\nLets plot the function for us to understand it better! As we have three parameters that vary, to study the function, we need a 3D plot.\n\n$$\nMSE(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\right) \\right)^2\n$$\n\nUsing same betas as above but more spaced out:\n\n```{r}\n#B0_values <- seq(10, 60, by = 20)\n#B1_values <- seq(0.5, 8, by = 2)\n\nB0_values <- seq(20, 120, by = 0.5)\nB1_values <- seq(-5, 10, by = 0.1)\n```\n\nWe can calculate the MSE for each combination (same way as we did by hand before) and store it in a matrix\n\n```{r}\n\ndf <- expand_grid(B0_values, B1_values ) %>% \n  rowwise() %>%\n  mutate(MSE = mse_practical(beta0 = B0_values, beta1 = B1_values, x = DataSmaller$age, y = DataSmaller$glucose, m=length(DataSmaller$glucose)))\n\nhead(df)\n```\n\n\n\nNow lets plot this function!\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(plotly)\nlibrary(htmlwidgets)\n\nwide_df <- df %>%\n  pivot_wider(\n    names_from = B1_values,  # Column names from B1_values\n    values_from = MSE        # Fill these columns with MSE values\n  )\n\n# Convert wide data to matrix for Plotly\nMSE_matrix <- as.matrix(wide_df[, -1])  # Exclude the first column (B0_values)\n\n# Extract B0 and B1 values\nB0_values <- sort(wide_df$B0_values)\nB1_values <- as.numeric(colnames(MSE_matrix))  # Column names become B1 values\n\n\n# If the rows of MSE_matrix need to be flipped, reorder them\n#MSE_matrix <- MSE_matrix[order(B0_values), ]\n\nplot_ly(\n  x = B1_values, \n  y = B0_values, \n  z = MSE_matrix,  # Use the reshaped matrix\n  type = \"surface\"\n) %>%\n  layout(\n    scene = list(\n      xaxis = list(title = \"B1_values\"),\n      yaxis = list(title = \"B0_values\"),\n      zaxis = list(title = \"MSE\")\n    )\n  )\n\n\n```\n\nWhat is the minimum MSE? This one! Roughly we can see that it corresponds to around x. What we also find from here is the idea that the minim point is the one at the bottom of the 3D curved surface.\n\n\n```{r}\nlibrary(plotly)\n\nmin(df$MSE)\n# Find the minimum point\nfilter(df, MSE == min(df$MSE))\n```\n\n\n```{r}\n\nfig <- plot_ly(\n  x = B1_values, \n  y = B0_values, \n  z = MSE_matrix,  # Use the reshaped matrix\n  type = \"surface\"\n) %>%\n  layout(\n    title = \"3D Surface Plot of MSE\",\n    scene = list(\n      xaxis = list(title = \"B1_values\"),\n      yaxis = list(title = \"B0_values\"),\n      zaxis = list(title = \"MSE\"), \n       camera = list(\n        eye = list(x = 2, y = 2, z = 1) # Adjust these values for the angle\n      )\n    )\n  )\n\n# Add the minimum point\nfig <- fig %>%\n  add_trace(\n    x = filter(df, MSE == min(df$MSE))$B1_values,\n    y = filter(df, MSE == min(df$MSE))$B0_values,\n    z = filter(df, MSE == min(df$MSE))$MSE,\n    mode = \"markers\",\n    type = \"scatter3d\",\n    marker = list(size = 10, color = \"red\"),\n    name = \"Minimum MSE\"\n  )\n\n# Render the plot\nfig\n\n```\n\n\n\n\nThese parameters are very similar to what we obtained from the `lm()` function (and would be the same if we had sampled the MSE at that accurate decimal point)\n\n```{r}\n\n\nlm(glucose ~ age, data = DataSmaller)\n\n\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"python":{"version":"r-reticulate"},"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"MSE_Change_FINAL.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","theme":"cosmo","title":"(3) Best Fit Line"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
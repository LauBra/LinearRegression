{
  "hash": "eba61f24f63b151df2069e6aa760c0b5",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(plot3D)\n```\n:::\n\n\n\n# (4) Gradient Descent\n\nGradient descent is a foundational **optimization** technique widely used in machine learning to find the **minimum** of a function. [The core idea is to iteratively adjust variables in the opposite direction of the gradient (slope) to minimize the function’s output]{style=\"color:blue;\"}.\n\n## The Gradient Descent Formula\n\nThe general update formula for gradient descent is:\n\n$$\nx_{n+1} = x_n - \\alpha f'(x_n) \n$$\n\nwhere:\n\n-   $x_n$ is the current guess\n-   $f'(x_n)$ is the gradient at $x_n$\n-   $\\alpha$ is the learning rate (a small positive constant that controls the step size)\n\nThe idea is that the steeper the slope, the larger the update, moving us closer to the minimum.\n\n## Single-Variable Gradient Descent\n\nLet’s replicate the slides seen in class and minimize a single-variable function, $f(x) = x^2$. Imagine we start with an initial guess that the minimum is at $x = -4$ (although we know this is not the true minimum, from class we have seen this happens at $x = 0$). We will iteratively improve this guess by calculating the derivative (gradient) and adjusting the guess.\n\n### Understanding the Derivative's Role\n\nIn gradient descent, the derivative (or gradient) tells us the slope of the function at any given point. Here’s the intuition:\n\n-   *If the derivative is positive*: The function is sloping upwards, so we should move \"downhill\" by decreasing our guess.\n\n-   *If the derivative is negative*: The function is sloping downwards, so we should move \"downhill\" by increasing our guess.\n\nThis approach ensures that we move towards the minimum at each step.\n\n### Example: Minimizing $f(x) = x^2$\n\nUsing function $f(x) = x^2$, first we have to calculate the derivative of this function $f'(x) = 2x$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Define the function and its derivative \n\nf <- function(x) {\n  \n  x^2\n}\n\nf_prime <- function(x) {\n  \n  2* x\n  \n}\n```\n:::\n\n\nThis is how they look:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n# Create a sequence of x values\nx_vals <- seq(-10, 10, length.out = 100)\n\n# Compute y values for both functions\ndata <- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           x       f_x f_prime_x\n1 -10.000000 100.00000 -20.00000\n2  -9.797980  96.00041 -19.59596\n3  -9.595960  92.08244 -19.19192\n4  -9.393939  88.24610 -18.78788\n5  -9.191919  84.49138 -18.38384\n6  -8.989899  80.81828 -17.97980\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nA <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x, color = \"f(x) = x^2\"), size = 1) +\n  #geom_line(aes(y = f_prime_x, color = \"f'(x) = 2x\"), size = 1) +\n  labs(title = \"Plot of f(x) = x^2 \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c(\"f(x) = x^2\" = \"blue\", \"f'(x) = 2x\" = \"red\")) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nB <- ggplot(data, aes(x = x)) +\n  #geom_line(aes(y = f_x, color = \"f(x) = x^2\"), size = 1) +\n  geom_line(aes(y = f_prime_x, color = \"f'(x) = 2x\"), size = 1) +\n  labs(title = \"Plot of  f'(x) = 2x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 100) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c(\"f(x) = x^2\" = \"blue\", \"f'(x) = 2x\" = \"red\")) +\n  theme_minimal()\n\nA + B\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set initial values\n\nalpha <- 0.8 # Learning rate \nx <- -10 # Initial guess\nx_store <- NULL\n```\n:::\n\n\nAs previosuly explained, the general update formula for gradient descent is:\n\n$$\nx_{n+1} = x_n - \\alpha f'(x_n) \n$$\n\nAs you can see, this code iteratively adjusts our guess $x$ based on the gradient at each step, converging towards the minimum. To find the minimum of a function, gradient descent updates the current guess x_n by a step that depends on the gradient (slope) at x_n.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gradient descent loop\n\nfor (i in 1:10) { \n  \n  gradient <- f_prime(x) \n  \n  \n  # Update parameters\n  x <- x - alpha * gradient\n\n  # Store values for plotting\n  x_store[i] <- x\n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n  }\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1 : x = 6 f(x) = 36 \nStep 2 : x = -3.6 f(x) = 12.96 \nStep 3 : x = 2.16 f(x) = 4.6656 \nStep 4 : x = -1.296 f(x) = 1.679616 \nStep 5 : x = 0.7776 f(x) = 0.6046618 \nStep 6 : x = -0.46656 f(x) = 0.2176782 \nStep 7 : x = 0.279936 f(x) = 0.07836416 \nStep 8 : x = -0.1679616 f(x) = 0.0282111 \nStep 9 : x = 0.100777 f(x) = 0.010156 \nStep 10 : x = -0.06046618 f(x) = 0.003656158 \n```\n\n\n:::\n:::\n\n\nThis code iteratively adjusts $x$ based on the gradient at each step, converging towards the minimum of f(x).\n\nPlotting it all together\n\n\n::: {.cell}\n\n```{.r .cell-code}\nApproximations <- data.frame(x = x_store, y = f(x_store), dy = f_prime(x_store)) %>%\n  add_rownames()# Assign a \"darkening\" group\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nApproximations$rowname <- as.numeric(Approximations$rowname )\n\nA <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^2 \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nB <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_prime_x),  color = \"blue\", size = 1) +\n  labs(title = \"Plot of  f'(x) = 2x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 100) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = dy, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nA + B\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n::: callout-note\n## Note\n\nA larger learning rate $\\alpha$ can make the steps too large, causing the algorithm to \"overshoot\" the minimum, while a smaller learning rate may result in slow convergence.\n:::\n\n## Your turn! \n\nUse gradient descent to minimize the following functions. For each function, start with different initial guesses and observe how the algorithm converges to a minimum.\n\n$$\nf(x) = x^2 + 3x + 5\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the function and its derivative \n\nf <- function(x) {\n  x^2 + 3*x + 5\n  \n}\n  \nf_prime <- function(x) {\n  \n  2*x + 3\n  \n}\n```\n:::\n\n\nWhy not plot it?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sequence of x values\nx_vals <- seq(-15, 15, length.out = 100)\n\n# Compute y values for both functions\ndata <- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x      f_x f_prime_x\n1 -15.00000 185.0000 -27.00000\n2 -14.69697 176.9100 -26.39394\n3 -14.39394 169.0037 -25.78788\n4 -14.09091 161.2810 -25.18182\n5 -13.78788 153.7420 -24.57576\n6 -13.48485 146.3866 -23.96970\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nA <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x, color = \"f(x) =  x^2 + 3*x + 5\"), size = 1) +\n  #geom_line(aes(y = f_prime_x, color = \"f'(x) = 2x + 1\"), size = 1) +\n  labs(title = \"Plot of f(x) =  x^2 + 3*x + 5\",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c(\"f(x) =  x^2 + 3*x + 5\" = \"blue\")) +\n  theme_minimal()\n\nB <- ggplot(data, aes(x = x)) +\n  #geom_line(aes(y = f_x, color = \"f(x) = x^2\"), size = 1) +\n  geom_line(aes(y = f_prime_x, color = \"f'(x) = 2*x + 3\"), size = 1) +\n  labs(title = \"Plot of  f'(x) = 2*x + 3\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-30, 100) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_color_manual(\"\", values = c( \"f'(x) = 2*x + 3\" = \"red\")) +\n  theme_minimal()\n\nA + B\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters\n\nalpha <- 0.01 \nx <- 2 # Initial guess\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gradient descent loop\n\nfor (i in 1:10) { \n  \n  x <- x - alpha * f_prime(x) \n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n  }\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1 : x = 1.93 f(x) = 14.5149 \nStep 2 : x = 1.8614 f(x) = 14.04901 \nStep 3 : x = 1.794172 f(x) = 13.60157 \nStep 4 : x = 1.728289 f(x) = 13.17185 \nStep 5 : x = 1.663723 f(x) = 12.75914 \nStep 6 : x = 1.600448 f(x) = 12.36278 \nStep 7 : x = 1.538439 f(x) = 11.98211 \nStep 8 : x = 1.477671 f(x) = 11.61652 \nStep 9 : x = 1.418117 f(x) = 11.26541 \nStep 10 : x = 1.359755 f(x) = 10.9282 \n```\n\n\n:::\n:::\n\n\n## Part 3: Local vs Global Minima\n\nGradient descent may not always reach the global minimum, especially if the function has multiple minima. The algorithm might \"get stuck\" in a local minimum, particularly if the initial guess is close to one of these minimum.\n\nConsider the function $f(x) = x^4 + x^3 - 2x^2$, which has both local and global minima.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) {\n  \n  x^4 + x^3 - 2*(x^2) \n}\n  \nf_prime <- function(x){\n  \n  4*(x^3) + 3*(x^2) - 4*x\n}\n```\n:::\n\n\nParameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.01 \nx <- 1\nx_store <- NULL\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:10) { \n  \n  \n  x <- x - alpha *f_prime(x) \n  \n  x_store[i] <- x\n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1 : x = 0.97 f(x) = -0.08383419 \nStep 2 : x = 0.9440661 f(x) = -0.1467667 \nStep 3 : x = 0.9214345 f(x) = -0.1948753 \nStep 4 : x = 0.9015272 f(x) = -0.2322205 \nStep 5 : x = 0.8838971 f(x) = -0.2615931 \nStep 6 : x = 0.8681921 f(x) = -0.2849583 \nStep 7 : x = 0.8541308 f(x) = -0.303729 \nStep 8 : x = 0.841485 f(x) = -0.3189397 \nStep 9 : x = 0.8300673 f(x) = -0.3313601 \nStep 10 : x = 0.8197226 f(x) = -0.3415714 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nApproximations <- data.frame(x = x_store, y = f(x_store), dy = f_prime(x_store)) %>%\n  add_rownames()# Assign a \"darkening\" group)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nApproximations$rowname <- as.numeric(Approximations$rowname )\n\n# Create a sequence of x values\nx_vals <- seq(-15, 15, length.out = 100)\n\n# Compute y values for both functions\ndata <- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x      f_x  f_prime_x\n1 -15.00000 46800.00 -12765.000\n2 -14.69697 43049.84 -11991.445\n3 -14.39394 39529.24 -11249.729\n4 -14.09091 36228.67 -10539.185\n5 -13.78788 33138.78  -9859.144\n6 -13.48485 30250.42  -9208.938\n```\n\n\n:::\n\n```{.r .cell-code}\nA_big <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n\n# Create a sequence of x values\nx_vals <- seq(-5, 5, length.out = 100)\n\n# Compute y values for both functions\ndata <- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x      f_x f_prime_x\n1 -5.000000 450.0000 -405.0000\n2 -4.898990 410.4284 -378.7088\n3 -4.797980 373.4548 -353.5559\n4 -4.696970 338.9655 -329.5168\n5 -4.595960 306.8491 -306.5666\n6 -4.494949 276.9970 -284.6806\n```\n\n\n:::\n\n```{.r .cell-code}\nA <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n   ylim(-5, 5) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nB <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_prime_x),  color = \"blue\", size = 1) +\n  labs(title = \"Plot of  f'(x) = 4*x^3 + 3*x^2 - 4*x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 90) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = dy, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n(A_big | A) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 61 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n(A | B )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 61 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\nWe see this function has 2 minimums! One is a local minimim and the other the global minimum. Depending on where we start (intialization point) we will end up in one or the other.\n\nExercise! Try different initial guesses, record which initial guess leads to global minimum.\n\nParameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.03\nx <- -0.5\nx_store <- NULL\nnumber_iterations <- 100\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:number_iterations) { \n  \n  \n  x <- x - alpha *f_prime(x) \n  \n  x_store[i] <- x\n  \n  cat(\"Step\", i, \": x =\", x, \"f(x) =\", f(x), \"\\n\") \n  \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1 : x = -0.5675 f(x) = -0.7231592 \nStep 2 : x = -0.642653 f(x) = -0.920852 \nStep 3 : x = -0.7250915 f(x) = -1.156317 \nStep 4 : x = -0.813674 f(x) = -1.424506 \nStep 5 : x = -0.9062562 f(x) = -1.712375 \nStep 6 : x = -0.9996069 f(x) = -1.998821 \nStep 7 : x = -1.08963 f(x) = -2.258633 \nStep 8 : x = -1.171997 f(x) = -2.470269 \nStep 9 : x = -1.243079 f(x) = -2.62357 \nStep 10 : x = -1.300817 f(x) = -2.722108 \nStep 11 : x = -1.345069 f(x) = -2.778691 \nStep 12 : x = -1.377285 f(x) = -2.808136 \nStep 13 : x = -1.39977 f(x) = -2.822284 \nStep 14 : x = -1.414967 f(x) = -2.828682 \nStep 15 : x = -1.425001 f(x) = -2.831453 \nStep 16 : x = -1.43152 f(x) = -2.832617 \nStep 17 : x = -1.43571 f(x) = -2.833097 \nStep 18 : x = -1.438384 f(x) = -2.833291 \nStep 19 : x = -1.440082 f(x) = -2.83337 \nStep 20 : x = -1.441158 f(x) = -2.833402 \nStep 21 : x = -1.441838 f(x) = -2.833414 \nStep 22 : x = -1.442267 f(x) = -2.833419 \nStep 23 : x = -1.442538 f(x) = -2.833421 \nStep 24 : x = -1.442709 f(x) = -2.833422 \nStep 25 : x = -1.442817 f(x) = -2.833422 \nStep 26 : x = -1.442885 f(x) = -2.833422 \nStep 27 : x = -1.442928 f(x) = -2.833422 \nStep 28 : x = -1.442955 f(x) = -2.833422 \nStep 29 : x = -1.442972 f(x) = -2.833422 \nStep 30 : x = -1.442982 f(x) = -2.833422 \nStep 31 : x = -1.442989 f(x) = -2.833422 \nStep 32 : x = -1.442993 f(x) = -2.833422 \nStep 33 : x = -1.442996 f(x) = -2.833422 \nStep 34 : x = -1.442998 f(x) = -2.833422 \nStep 35 : x = -1.442999 f(x) = -2.833422 \nStep 36 : x = -1.442999 f(x) = -2.833422 \nStep 37 : x = -1.443 f(x) = -2.833422 \nStep 38 : x = -1.443 f(x) = -2.833422 \nStep 39 : x = -1.443 f(x) = -2.833422 \nStep 40 : x = -1.443 f(x) = -2.833422 \nStep 41 : x = -1.443 f(x) = -2.833422 \nStep 42 : x = -1.443 f(x) = -2.833422 \nStep 43 : x = -1.443 f(x) = -2.833422 \nStep 44 : x = -1.443 f(x) = -2.833422 \nStep 45 : x = -1.443 f(x) = -2.833422 \nStep 46 : x = -1.443 f(x) = -2.833422 \nStep 47 : x = -1.443 f(x) = -2.833422 \nStep 48 : x = -1.443 f(x) = -2.833422 \nStep 49 : x = -1.443 f(x) = -2.833422 \nStep 50 : x = -1.443 f(x) = -2.833422 \nStep 51 : x = -1.443 f(x) = -2.833422 \nStep 52 : x = -1.443 f(x) = -2.833422 \nStep 53 : x = -1.443 f(x) = -2.833422 \nStep 54 : x = -1.443 f(x) = -2.833422 \nStep 55 : x = -1.443 f(x) = -2.833422 \nStep 56 : x = -1.443 f(x) = -2.833422 \nStep 57 : x = -1.443 f(x) = -2.833422 \nStep 58 : x = -1.443 f(x) = -2.833422 \nStep 59 : x = -1.443 f(x) = -2.833422 \nStep 60 : x = -1.443 f(x) = -2.833422 \nStep 61 : x = -1.443 f(x) = -2.833422 \nStep 62 : x = -1.443 f(x) = -2.833422 \nStep 63 : x = -1.443 f(x) = -2.833422 \nStep 64 : x = -1.443 f(x) = -2.833422 \nStep 65 : x = -1.443 f(x) = -2.833422 \nStep 66 : x = -1.443 f(x) = -2.833422 \nStep 67 : x = -1.443 f(x) = -2.833422 \nStep 68 : x = -1.443 f(x) = -2.833422 \nStep 69 : x = -1.443 f(x) = -2.833422 \nStep 70 : x = -1.443 f(x) = -2.833422 \nStep 71 : x = -1.443 f(x) = -2.833422 \nStep 72 : x = -1.443 f(x) = -2.833422 \nStep 73 : x = -1.443 f(x) = -2.833422 \nStep 74 : x = -1.443 f(x) = -2.833422 \nStep 75 : x = -1.443 f(x) = -2.833422 \nStep 76 : x = -1.443 f(x) = -2.833422 \nStep 77 : x = -1.443 f(x) = -2.833422 \nStep 78 : x = -1.443 f(x) = -2.833422 \nStep 79 : x = -1.443 f(x) = -2.833422 \nStep 80 : x = -1.443 f(x) = -2.833422 \nStep 81 : x = -1.443 f(x) = -2.833422 \nStep 82 : x = -1.443 f(x) = -2.833422 \nStep 83 : x = -1.443 f(x) = -2.833422 \nStep 84 : x = -1.443 f(x) = -2.833422 \nStep 85 : x = -1.443 f(x) = -2.833422 \nStep 86 : x = -1.443 f(x) = -2.833422 \nStep 87 : x = -1.443 f(x) = -2.833422 \nStep 88 : x = -1.443 f(x) = -2.833422 \nStep 89 : x = -1.443 f(x) = -2.833422 \nStep 90 : x = -1.443 f(x) = -2.833422 \nStep 91 : x = -1.443 f(x) = -2.833422 \nStep 92 : x = -1.443 f(x) = -2.833422 \nStep 93 : x = -1.443 f(x) = -2.833422 \nStep 94 : x = -1.443 f(x) = -2.833422 \nStep 95 : x = -1.443 f(x) = -2.833422 \nStep 96 : x = -1.443 f(x) = -2.833422 \nStep 97 : x = -1.443 f(x) = -2.833422 \nStep 98 : x = -1.443 f(x) = -2.833422 \nStep 99 : x = -1.443 f(x) = -2.833422 \nStep 100 : x = -1.443 f(x) = -2.833422 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nApproximations <- data.frame(x = x_store, y = f(x_store), dy = f_prime(x_store)) %>%\n  add_rownames()# Assign a \"darkening\" group)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nApproximations$rowname <- as.numeric(Approximations$rowname )\n\n# Create a sequence of x values\nx_vals <- seq(-15, 15, length.out = 100)\n\n# Compute y values for both functions\ndata <- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x      f_x  f_prime_x\n1 -15.00000 46800.00 -12765.000\n2 -14.69697 43049.84 -11991.445\n3 -14.39394 39529.24 -11249.729\n4 -14.09091 36228.67 -10539.185\n5 -13.78788 33138.78  -9859.144\n6 -13.48485 30250.42  -9208.938\n```\n\n\n:::\n\n```{.r .cell-code}\nA_big <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n\n# Create a sequence of x values\nx_vals <- seq(-5, 5, length.out = 100)\n\n# Compute y values for both functions\ndata <- data.frame(\n  x = x_vals,\n  f_x = f(x_vals),\n  f_prime_x = f_prime(x_vals)\n)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x      f_x f_prime_x\n1 -5.000000 450.0000 -405.0000\n2 -4.898990 410.4284 -378.7088\n3 -4.797980 373.4548 -353.5559\n4 -4.696970 338.9655 -329.5168\n5 -4.595960 306.8491 -306.5666\n6 -4.494949 276.9970 -284.6806\n```\n\n\n:::\n\n```{.r .cell-code}\nA <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\", size = 1) +\n  labs(title = \"Plot of f(x) = x^4 + x^3 - 2*x^2  \",\n       x = \"x\",\n       y = \"f(x)\") +\n   ylim(-5, 5) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = y, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\nB <- ggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_prime_x),  color = \"blue\", size = 1) +\n  labs(title = \"Plot of  f'(x) = 4*x^3 + 3*x^2 - 4*x\",\n       x = \"x\",\n       y = \"f'(x)\") +\n  ylim(-20, 90) +  \n  geom_hline(yintercept = 0, color = \"black\") +  # Add horizontal line at y = 0\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_point(data = Approximations, aes(x = x, y = dy, fill = rowname), size = 3,pch=21,colour = \"black\", show.legend = FALSE) +\n  scale_fill_gradient(low = \"yellow\", high = \"red\", na.value = NA) +\n  theme_minimal()\n\n(A_big | A) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 61 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n(A | B )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 61 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n:::\n\n\nWhat happens at initial point x = 0? And if you increase the learning rate a lot? Does it mean it gets to the minimum faster? But which one? What is another parameter you can modify?..... ITERATIONS!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Try out any other modification!\n```\n:::\n\n\n\nAs you have seen:\n\nThe choice of learning rate $\\alpha$ is crucial:\n\n-   If $\\alpha$ too large, the algorithm might oscillate and fail to converge.\n\n-   If $\\alpha$ too slow, requiring more iterations.\n\nExample of high learning rate:\n\nSetting $\\alpha = 0.5$\n\n## Part 4: Multivariable Gradient Descent\n\nMultivariable gradient descent is an extension of the single-variable case. Instead of using a single derivative, we calculate the gradient vector, which consists of the partial derivatives of the function with respect to each variable.\n\nConsider the function:\n\n$$\nf(x, y) = xsin(y) + x^2\n$$\n\nThe gradient of this function is:\n\n$$\n\\nabla f(x, y) = \\left\\langle \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right\\rangle = \\langle \\sin(y) + 2x, x \\cos(y) \\rangle\n$$\n\nLet's apply gradient descent starting from an initial guess of $\\vec{x} = (1, 2)$ with a learning rate of $\\alpha = 0.01$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the function and its partial derivatives \n\nf <- function(x, y) {\n  x* sin(y) + x^2 \n}\n  \nf_x <- function(x, y) {\n  \n  sin(y) + 2*x \n  \n}\n  \n\nf_y <- function(x, y){\n  \n  x*cos(y)\n\n}\n```\n:::\n\n\nAnother way to do it in R is using the function Deriv()\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Deriv) \n\n\n# Define the function f(x, y) = x* sin(y) + x^2\n\nf <- function(x, y) { \n  x*sin(y) + x^2 \n  }\n\n# Compute the gradient symbolically (exact expressions)\n\ngrad_f <- Deriv(expression(x*sin(y) + x^2), c(\"x\", \"y\"))\n\nprint(grad_f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nexpression(c(x = 2 * x + sin(y), y = x * cos(y)))\n```\n\n\n:::\n\n```{.r .cell-code}\n#expression(c(x = 2 * x + sin(y), y = x * cos(y))) Computes the partial derivatives for you!!!!!\n\ngradient <- function(x, y) { \n  \n  eval(grad_f) \n  }\n```\n:::\n\n\nAs you can see is the partial derivatives above written, directly calculated\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initial parameters\n\nalpha <- 0.01 # Learning rate \niterations <- 100 # Number of iterations \nx <- 1 # Initial guess for x \ny <- 2 # Initial guess for y\n\n# Store results for plotting\n\nresults <- data.frame(Iteration = 0, x = x, y = y, f_value = f(x, y))\n\n# Gradient descent loop\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:iterations) { \n  \n  grad <- gradient(x = x, y = y)\n  \n  # Evaluate gradient \n  x <- x - alpha* grad[1] \n  y <- y - alpha * grad[2] \n  results <- rbind(results, data.frame(Iteration = i, x = x, y = y, f_value = f(x, y))) \n  \n # would be the same as:\n  #x <- x - alpha*f_x(x, y) \n  #y <- y - alpha* f_y(x, y) \n  \n  \n  }\n\n# Display first few iterations\n\nhead(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Iteration         x        y  f_value\n1          0 1.0000000 2.000000 1.909297\nx          1 0.9709070 2.004161 1.823815\nx1         2 0.9424133 2.008239 1.741817\nx2         3 0.9145067 2.012231 1.663164\nx3         4 0.8871751 2.016138 1.587723\nx4         5 0.8604070 2.019960 1.515364\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Generate grid data for 3D surface plot \n\nx_vals <- seq(-2, 2, length.out = 50) \ny_vals <- seq(-1, 3, length.out = 50) \nz_vals <- outer(x_vals, y_vals, Vectorize(f)) #evaluate x and y values in function f\n\n# 3D plot\n\npersp3D(x = x_vals, y = y_vals, z = z_vals, col = \"lightblue\", theta = 30, phi = 20, expand = 0.6, shade = 0.5, main = \"Gradient Descent Path on f(x, y) = x*sin(y) + x^2\", xlab = \"x\", ylab = \"y\", zlab = \"f(x, y)\")\n\n# Overlay gradient descent path\n\npoints3D(results$x, results$y, results$f_value, col = \"red\", pch = 19, add = TRUE)\nlines3D(results$x, results$y, results$f_value, col = \"red\", add = TRUE)\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\nAs expected, the value of the function is being minimized at each iteration!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the value of f(x, y) over iterations \n\nggplot(results, aes(x = Iteration, y = f_value)) + geom_line(color = \"blue\") + labs( title = \"Convergence of Gradient Descent on f(x, y) = x*sin(y) + x^2\", x = \"Iteration\", y = \"f(x, y)\" ) + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n## Final Remarks\n\nGradient descent is a versatile optimization technique, but it’s not without limitations:\n\n* It may converge to local minima rather than the global minimum. It is sensitive to the choice of learning rate and initial guess.\n\n* Variants of gradient descent, like stochastic gradient descent (SGD) and momentum-based methods, are often used to address these issues in large-scale machine learning tasks. Understanding and experimenting with gradient descent is crucial for developing an intuition about optimization in machine learning and algorithms. \n\n# Try it out with our own MSE, from the previous exercises\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench)\n\n# Load the dataset\ndata(\"PimaIndiansDiabetes\")\n\n# Select the first 80 rows and extract the variables\ndata_subset <- PimaIndiansDiabetes[1:80, ]\nx <- data_subset$age\ny <- data_subset$glucose\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gradient descent parameters\nalpha <- 0.0001  # Learning rate\niterations <- 1000  # Number of iterations\nm <- -3  # Initial guess for slope\nc <- 55  # Initial guess for intercept\n\n# Lists to store m, c, and MSE values for plotting\nm_path <- numeric(iterations)\nc_path <- numeric(iterations)\nmse_history <- numeric(iterations)\n```\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the MSE function\nmse <- function(m, c, x, y) {\n  \n  y_pred <- m * x + c\n  mean((y - y_pred)^2)\n  \n  #same thing: (1 / n) * sum((y - (m * x + c))^2)\n}\n\n# Define the gradients of MSE with respect to m and c\nmse_gradient_m <- function(m, c, x, y) {\n  -2 / length(y) * sum(x * (y - (m * x + c)))\n}\n\nmse_gradient_c <- function(m, c, x, y) {\n  -2 / length(y) * sum(y - (m * x + c))\n}\n\n#Remember can also apply the Deriv function \n\ngrad_f <- Deriv(expression((1 / n)* sum((y - (m * x + c))^2)), c(\"m\", \"c\"))\n\nprint(grad_f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nexpression({\n    .e2 <- y - (c + m * x)\n    c(m = sum(-(2 * (x * .e2)))/n, c = sum(-(2 * .e2))/n)\n})\n```\n\n\n:::\n:::\n\nUnsure where the derivatives come from or how to calculate them?Go to the OLS_Derivation exercise. \n\nNow, lets perform gradient descent: \n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform gradient descent\nfor (i in 1:iterations) {\n  # Compute gradients\n  grad_m <- mse_gradient_m(m, c, x, y)\n  grad_c <- mse_gradient_c(m, c, x, y)\n  \n  # Update parameters\n  m <- m - alpha * grad_m\n  c <- c - alpha * grad_c\n  \n  # Store values for plotting\n  m_path[i] <- m\n  c_path[i] <- c\n  mse_history[i] <- mse(m, c, x, y)\n  \n  # Print progress every 100 iterations\n  if (i %% 100 == 0) {\n    cat(\"Iteration:\", i, \"Slope (m):\", m, \"Intercept (c):\", c, \"MSE:\", mse_history[i], \"\\n\")\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIteration: 100 Slope (m): 1.807502 Intercept (c): 55.15779 MSE: 1051.732 \nIteration: 200 Slope (m): 1.806631 Intercept (c): 55.19157 MSE: 1051.618 \nIteration: 300 Slope (m): 1.805762 Intercept (c): 55.22528 MSE: 1051.505 \nIteration: 400 Slope (m): 1.804895 Intercept (c): 55.25893 MSE: 1051.391 \nIteration: 500 Slope (m): 1.804029 Intercept (c): 55.29252 MSE: 1051.278 \nIteration: 600 Slope (m): 1.803165 Intercept (c): 55.32605 MSE: 1051.166 \nIteration: 700 Slope (m): 1.802302 Intercept (c): 55.35952 MSE: 1051.054 \nIteration: 800 Slope (m): 1.801441 Intercept (c): 55.39293 MSE: 1050.942 \nIteration: 900 Slope (m): 1.800581 Intercept (c): 55.42627 MSE: 1050.831 \nIteration: 1000 Slope (m): 1.799723 Intercept (c): 55.45956 MSE: 1050.72 \n```\n\n\n:::\n:::\n\n\nOptimized parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0(\"Slope:\" ,m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope:1.79972281513455\"\n```\n\n\n:::\n\n```{.r .cell-code}\npaste0(\"Slope:\" ,c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope:55.4595574631306\"\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the data points and the fitted line\nggplot(data_subset, aes(x = age, y = glucose)) +\n  geom_point() +\n  geom_abline(intercept = c, slope = m, color = \"blue\", size = 1) +\n  labs(title = \"Linear Fit using Gradient Descent\", x = \"Age\", y = \"Glucose\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot MSE over iterations\nmse_df <- data.frame(iteration = 1:iterations, MSE = mse_history)\nggplot(mse_df, aes(x = iteration, y = MSE)) +\n  geom_line(color = \"red\") +\n  labs(title = \"MSE over Iterations\", x = \"Iteration\", y = \"Mean Squared Error\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-30-2.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid of values for m and c\nm_values <- seq(-5, 5, length.out = 50)\nc_values <- seq(0, 200, length.out = 50)\n\n# Initialize a matrix to store MSE values\nmse_matrix <- outer(m_values, c_values, Vectorize(function(m, c) mse(m, c, x, y)))\n\n# Plot the MSE surface\npersp3D(x = m_values, y = c_values, z = mse_matrix, \n        theta = 45, phi = 0, \n        xlab = \"Slope (m)\", ylab = \"Intercept (c)\", zlab = \"MSE\",\n        main = \"MSE Surface with Gradient Descent Path\",ticktype = \"detailed\" )\n\n# Add the gradient descent path\npoints3D(m_path, c_path, z = sapply(1:iterations, function(i) mse(m_path[i], c_path[i], x, y)), \n         col = \"red\", pch = 20, add = TRUE, cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](GradientDescent2_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nExercise Questions\n\n1.  Experiment with the Learning Rate Try changing the value of alpha (learning rate) to see its effect on convergence. Question: What happens if alpha is too high? Does the MSE converge smoothly, or does it oscillate? Question: What happens if alpha is too low? How does it affect the number of iterations required to reach a stable value?\n2.  Change the Initial Guess Try different initial values for m. For example, use m = 5 or m = -3. Question: Does the algorithm converge to the same solution? How does the initial value of m affect the convergence?\n3.  Extend to Optimize Both m and c Modify the code to perform gradient descent on both the slope (m) and intercept (c). Hint: You'll need to add a derivative function for c and update c in each iteration as well. Question: How does optimizing both m and c simultaneously compare to optimizing only m?\n",
    "supporting": [
      "GradientDescent2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}